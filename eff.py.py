# -*- coding: utf-8 -*-
"""xNN_inverted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m1MZRi32Twa4DLMvQaldmL1n43N3ol1i
"""

################################################################################
#
# LOGISTICS
#
#    <TO DO: first and last name as in eLearning>
#     An Yu Liu
#    <TO DO: UTD ID>
#     AXL180015
#    <TO DO: this comment block is included in each file that is submitted:
#    eff.py, eff_se.py (if done) and eff_se_cond.py (if done)>
#    eff.py, eff_se.py
# FILE
#
#    eff.py | eff_se.py | eff_se_cond.py
#
# DESCRIPTION
#
#    Grade = eff.py grade (max 90) + eff_se.py grade (max 10) + eff_se_cond.py grade (max 10)
#
#    A PyTorch implementation of the network described in section 3 of
#    xNNs_Project_002_NetworksPaper.doc/pdf trained in Google Colaboratory using
#    a GPU instance (Runtime - Change runtime type - Hardware accelerator - GPU)
#
# INSTRUCTIONS
#
#    1. a. eff.py
#
#          Complete all <TO DO: ...> code portions of this file to design and
#          train the network in table 1 of the paper with the standard inverted
#          residual building block in fig 2a and report the results
#
#    1. b. eff_se.py
#
#          Starting from eff.py, create a SE block per fig 3 in the paper and
#          add it to the inverted residual block per fig 2b in the paper to
#          create a SE enhanced building block; design and train the network
#          in table 1 with the SE enhanced building block and report the same
#          results as in the standard building block case
#
#    1. c. eff_se_cond.py
#
#          Starting from eff_se.py, create a conditional conv operation per
#          fig 4 in the paper and replace the building block convolutions with
#          the conditional convolution operation to create a SE and conditional
#          convolution enhanced building block per fig 2c in the paper; design
#          and train the network in table 1 with the SE and conditional
#          convolution enhanced building block and report the same results as in
#          the standard building block case; it may be required to reduce the
#          batch size in this case if there is an out of memory error
#
#    2. Cut and paste the text output generated during training showing the per
#       epoch statistics (for all networks trained: standard, SE enhanced and
#       SE and conditional convolution enhanced)
#
#       <TO DO: cut and paste per epoch statistics here>
#
# stand:
# Using 1 GPU(s)
# Epoch   0 Time     18.6 lr = 0.002000 avg loss = 0.017991
# Epoch   0 Time     18.6 lr = 0.002000 avg loss = 0.017923
# Epoch   0 Time     18.7 lr = 0.002000 avg loss = 0.017859
# Epoch   0 Time     18.7 lr = 0.002000 avg loss = 0.017784
# Epoch   0 Time     18.9 lr = 0.002000 avg loss = 0.017688
# Epoch   0 Time     19.0 lr = 0.002000 avg loss = 0.017575
# Epoch   0 Time     19.1 lr = 0.002000 avg loss = 0.017456
# Epoch   0 Time     18.8 lr = 0.002000 avg loss = 0.017333
# Epoch   0 Time     18.9 lr = 0.002000 avg loss = 0.017217
# Epoch   0 Time     19.1 lr = 0.002000 avg loss = 0.017105
# Epoch   0 Time     19.0 lr = 0.002000 avg loss = 0.016996
# Epoch   0 Time     19.0 lr = 0.002000 avg loss = 0.016888
# Epoch   0 Time    242.0 lr = 0.002000 avg loss = 0.016824 accuracy =  8.54
# Epoch   1 Time     34.4 lr = 0.041600 avg loss = 0.015497
# Epoch   1 Time     18.6 lr = 0.041600 avg loss = 0.015051
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.014637
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.014355
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.014117
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.013904
# Epoch   1 Time     18.6 lr = 0.041600 avg loss = 0.013727
# Epoch   1 Time     18.6 lr = 0.041600 avg loss = 0.013546
# Epoch   1 Time     18.6 lr = 0.041600 avg loss = 0.013382
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.013240
# Epoch   1 Time     18.7 lr = 0.041600 avg loss = 0.013089
# Epoch   1 Time     18.6 lr = 0.041600 avg loss = 0.012961
# Epoch   1 Time    239.3 lr = 0.041600 avg loss = 0.012883 accuracy = 28.56
# Epoch   2 Time     34.2 lr = 0.081200 avg loss = 0.011806
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011775
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011668
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011569
# Epoch   2 Time     18.6 lr = 0.081200 avg loss = 0.011472
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011372
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011282
# Epoch   2 Time     18.7 lr = 0.081200 avg loss = 0.011191
# Epoch   2 Time     18.8 lr = 0.081200 avg loss = 0.011120
# Epoch   2 Time     18.8 lr = 0.081200 avg loss = 0.011024
# Epoch   2 Time     18.6 lr = 0.081200 avg loss = 0.010953
# Epoch   2 Time     18.6 lr = 0.081200 avg loss = 0.010892
# Epoch   2 Time    239.7 lr = 0.081200 avg loss = 0.010845 accuracy = 38.14
# Epoch   3 Time     34.0 lr = 0.120800 avg loss = 0.010327
# Epoch   3 Time     18.7 lr = 0.120800 avg loss = 0.010357
# Epoch   3 Time     18.7 lr = 0.120800 avg loss = 0.010297
# Epoch   3 Time     18.7 lr = 0.120800 avg loss = 0.010248
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.010189
# Epoch   3 Time     18.7 lr = 0.120800 avg loss = 0.010132
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.010057
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.010017
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.009981
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.009932
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.009889
# Epoch   3 Time     18.6 lr = 0.120800 avg loss = 0.009849
# Epoch   3 Time    239.1 lr = 0.120800 avg loss = 0.009819 accuracy = 38.44
# Epoch   4 Time     34.2 lr = 0.160400 avg loss = 0.009389
# Epoch   4 Time     18.5 lr = 0.160400 avg loss = 0.009383
# Epoch   4 Time     18.6 lr = 0.160400 avg loss = 0.009385
# Epoch   4 Time     18.7 lr = 0.160400 avg loss = 0.009355
# Epoch   4 Time     18.6 lr = 0.160400 avg loss = 0.009346
# Epoch   4 Time     18.7 lr = 0.160400 avg loss = 0.009327
# Epoch   4 Time     18.8 lr = 0.160400 avg loss = 0.009302
# Epoch   4 Time     18.8 lr = 0.160400 avg loss = 0.009262
# Epoch   4 Time     18.6 lr = 0.160400 avg loss = 0.009216
# Epoch   4 Time     18.7 lr = 0.160400 avg loss = 0.009171
# Epoch   4 Time     18.6 lr = 0.160400 avg loss = 0.009134
# Epoch   4 Time     18.8 lr = 0.160400 avg loss = 0.009109
# Epoch   4 Time    239.6 lr = 0.160400 avg loss = 0.009090 accuracy = 45.72
# Epoch   5 Time     34.0 lr = 0.200000 avg loss = 0.008748
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008805
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008731
# Epoch   5 Time     18.7 lr = 0.200000 avg loss = 0.008749
# Epoch   5 Time     18.7 lr = 0.200000 avg loss = 0.008689
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008688
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008670
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008652
# Epoch   5 Time     18.5 lr = 0.200000 avg loss = 0.008632
# Epoch   5 Time     18.7 lr = 0.200000 avg loss = 0.008617
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008599
# Epoch   5 Time     18.6 lr = 0.200000 avg loss = 0.008581
# Epoch   5 Time    239.0 lr = 0.200000 avg loss = 0.008570 accuracy = 47.28
# Epoch   6 Time     34.1 lr = 0.199795 avg loss = 0.008159
# Epoch   6 Time     18.6 lr = 0.199795 avg loss = 0.008070
# Epoch   6 Time     18.6 lr = 0.199795 avg loss = 0.008121
# Epoch   6 Time     18.6 lr = 0.199795 avg loss = 0.008087
# Epoch   6 Time     18.6 lr = 0.199795 avg loss = 0.008066
# Epoch   6 Time     18.8 lr = 0.199795 avg loss = 0.008047
# Epoch   6 Time     18.8 lr = 0.199795 avg loss = 0.008042
# Epoch   6 Time     18.7 lr = 0.199795 avg loss = 0.008031
# Epoch   6 Time     18.8 lr = 0.199795 avg loss = 0.008030
# Epoch   6 Time     18.9 lr = 0.199795 avg loss = 0.008016
# Epoch   6 Time     18.8 lr = 0.199795 avg loss = 0.008000
# Epoch   6 Time     18.8 lr = 0.199795 avg loss = 0.007996
# Epoch   6 Time    240.2 lr = 0.199795 avg loss = 0.007987 accuracy = 51.08
# Epoch   7 Time     34.3 lr = 0.199180 avg loss = 0.007586
# Epoch   7 Time     18.8 lr = 0.199180 avg loss = 0.007636
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007620
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007625
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007616
# Epoch   7 Time     18.6 lr = 0.199180 avg loss = 0.007613
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007620
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007630
# Epoch   7 Time     18.6 lr = 0.199180 avg loss = 0.007628
# Epoch   7 Time     18.5 lr = 0.199180 avg loss = 0.007621
# Epoch   7 Time     18.7 lr = 0.199180 avg loss = 0.007623
# Epoch   7 Time     19.1 lr = 0.199180 avg loss = 0.007609
# Epoch   7 Time    240.3 lr = 0.199180 avg loss = 0.007606 accuracy = 49.08
# Epoch   8 Time     34.6 lr = 0.198158 avg loss = 0.007210
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007309
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007336
# Epoch   8 Time     18.9 lr = 0.198158 avg loss = 0.007338
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007344
# Epoch   8 Time     18.6 lr = 0.198158 avg loss = 0.007322
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007326
# Epoch   8 Time     18.8 lr = 0.198158 avg loss = 0.007346
# Epoch   8 Time     18.6 lr = 0.198158 avg loss = 0.007334
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007328
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007329
# Epoch   8 Time     18.7 lr = 0.198158 avg loss = 0.007328
# Epoch   8 Time    240.2 lr = 0.198158 avg loss = 0.007325 accuracy = 53.70
# Epoch   9 Time     34.3 lr = 0.196733 avg loss = 0.007085
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007028
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007063
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007059
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007049
# Epoch   9 Time     18.9 lr = 0.196733 avg loss = 0.007060
# Epoch   9 Time     18.9 lr = 0.196733 avg loss = 0.007068
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007065
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007066
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007062
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007073
# Epoch   9 Time     18.8 lr = 0.196733 avg loss = 0.007068
# Epoch   9 Time    241.5 lr = 0.196733 avg loss = 0.007067 accuracy = 55.26
# Epoch  10 Time     34.3 lr = 0.194911 avg loss = 0.006919
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006862
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006918
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006901
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006862
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006873
# Epoch  10 Time     18.8 lr = 0.194911 avg loss = 0.006879
# Epoch  10 Time     18.7 lr = 0.194911 avg loss = 0.006872
# Epoch  10 Time     18.7 lr = 0.194911 avg loss = 0.006868
# Epoch  10 Time     18.6 lr = 0.194911 avg loss = 0.006857
# Epoch  10 Time     18.6 lr = 0.194911 avg loss = 0.006859
# Epoch  10 Time     18.7 lr = 0.194911 avg loss = 0.006856
# Epoch  10 Time    240.4 lr = 0.194911 avg loss = 0.006862 accuracy = 55.60
# Epoch  11 Time     34.2 lr = 0.192699 avg loss = 0.006698
# Epoch  11 Time     18.8 lr = 0.192699 avg loss = 0.006721
# Epoch  11 Time     18.7 lr = 0.192699 avg loss = 0.006701
# Epoch  11 Time     18.6 lr = 0.192699 avg loss = 0.006713
# Epoch  11 Time     18.7 lr = 0.192699 avg loss = 0.006706
# Epoch  11 Time     18.7 lr = 0.192699 avg loss = 0.006703
# Epoch  11 Time     19.1 lr = 0.192699 avg loss = 0.006698
# Epoch  11 Time     19.1 lr = 0.192699 avg loss = 0.006712
# Epoch  11 Time     18.8 lr = 0.192699 avg loss = 0.006713
# Epoch  11 Time     18.8 lr = 0.192699 avg loss = 0.006702
# Epoch  11 Time     19.0 lr = 0.192699 avg loss = 0.006707
# Epoch  11 Time     18.9 lr = 0.192699 avg loss = 0.006700
# Epoch  11 Time    241.5 lr = 0.192699 avg loss = 0.006705 accuracy = 57.02
# Epoch  12 Time     34.5 lr = 0.190107 avg loss = 0.006669
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006593
# Epoch  12 Time     18.9 lr = 0.190107 avg loss = 0.006593
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006574
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006565
# Epoch  12 Time     18.9 lr = 0.190107 avg loss = 0.006575
# Epoch  12 Time     18.9 lr = 0.190107 avg loss = 0.006574
# Epoch  12 Time     18.9 lr = 0.190107 avg loss = 0.006570
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006563
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006559
# Epoch  12 Time     18.8 lr = 0.190107 avg loss = 0.006564
# Epoch  12 Time     18.9 lr = 0.190107 avg loss = 0.006561
# Epoch  12 Time    241.8 lr = 0.190107 avg loss = 0.006561 accuracy = 60.36
# Epoch  13 Time     34.5 lr = 0.187145 avg loss = 0.006408
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006417
# Epoch  13 Time     18.8 lr = 0.187145 avg loss = 0.006407
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006373
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006399
# Epoch  13 Time     18.8 lr = 0.187145 avg loss = 0.006421
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006414
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006416
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006419
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006437
# Epoch  13 Time     18.6 lr = 0.187145 avg loss = 0.006447
# Epoch  13 Time     18.7 lr = 0.187145 avg loss = 0.006446
# Epoch  13 Time    240.2 lr = 0.187145 avg loss = 0.006436 accuracy = 60.92
# Epoch  14 Time     34.0 lr = 0.183825 avg loss = 0.006233
# Epoch  14 Time     18.7 lr = 0.183825 avg loss = 0.006241
# Epoch  14 Time     18.7 lr = 0.183825 avg loss = 0.006249
# Epoch  14 Time     18.6 lr = 0.183825 avg loss = 0.006257
# Epoch  14 Time     18.7 lr = 0.183825 avg loss = 0.006275
# Epoch  14 Time     18.7 lr = 0.183825 avg loss = 0.006302
# Epoch  14 Time     18.7 lr = 0.183825 avg loss = 0.006316
# Epoch  14 Time     18.5 lr = 0.183825 avg loss = 0.006317
# Epoch  14 Time     18.5 lr = 0.183825 avg loss = 0.006301
# Epoch  14 Time     18.6 lr = 0.183825 avg loss = 0.006298
# Epoch  14 Time     18.5 lr = 0.183825 avg loss = 0.006295
# Epoch  14 Time     18.5 lr = 0.183825 avg loss = 0.006298
# Epoch  14 Time    238.5 lr = 0.183825 avg loss = 0.006302 accuracy = 58.58
# Epoch  15 Time     33.8 lr = 0.180161 avg loss = 0.006086
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006161
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006130
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006152
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006165
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006163
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006181
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006178
# Epoch  15 Time     18.5 lr = 0.180161 avg loss = 0.006177
# Epoch  15 Time     18.4 lr = 0.180161 avg loss = 0.006187
# Epoch  15 Time     18.4 lr = 0.180161 avg loss = 0.006185
# Epoch  15 Time     18.6 lr = 0.180161 avg loss = 0.006191
# Epoch  15 Time    237.2 lr = 0.180161 avg loss = 0.006195 accuracy = 59.36
# Epoch  16 Time     33.7 lr = 0.176168 avg loss = 0.006070
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006002
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006030
# Epoch  16 Time     18.5 lr = 0.176168 avg loss = 0.006031
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006032
# Epoch  16 Time     18.5 lr = 0.176168 avg loss = 0.006057
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006059
# Epoch  16 Time     18.5 lr = 0.176168 avg loss = 0.006072
# Epoch  16 Time     18.3 lr = 0.176168 avg loss = 0.006065
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006072
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006078
# Epoch  16 Time     18.4 lr = 0.176168 avg loss = 0.006076
# Epoch  16 Time    236.5 lr = 0.176168 avg loss = 0.006073 accuracy = 60.36
# Epoch  17 Time     33.8 lr = 0.171863 avg loss = 0.005924
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005847
# Epoch  17 Time     18.3 lr = 0.171863 avg loss = 0.005825
# Epoch  17 Time     18.5 lr = 0.171863 avg loss = 0.005874
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005898
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005907
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005928
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005955
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005961
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005969
# Epoch  17 Time     18.4 lr = 0.171863 avg loss = 0.005975
# Epoch  17 Time     18.3 lr = 0.171863 avg loss = 0.005976
# Epoch  17 Time    235.9 lr = 0.171863 avg loss = 0.005977 accuracy = 62.70
# Epoch  18 Time     33.5 lr = 0.167263 avg loss = 0.005834
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005839
# Epoch  18 Time     18.3 lr = 0.167263 avg loss = 0.005856
# Epoch  18 Time     18.5 lr = 0.167263 avg loss = 0.005866
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005863
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005869
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005881
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005887
# Epoch  18 Time     18.3 lr = 0.167263 avg loss = 0.005890
# Epoch  18 Time     18.3 lr = 0.167263 avg loss = 0.005894
# Epoch  18 Time     18.4 lr = 0.167263 avg loss = 0.005900
# Epoch  18 Time     18.3 lr = 0.167263 avg loss = 0.005890
# Epoch  18 Time    235.4 lr = 0.167263 avg loss = 0.005888 accuracy = 61.44
# Epoch  19 Time     33.5 lr = 0.162387 avg loss = 0.005750
# Epoch  19 Time     18.3 lr = 0.162387 avg loss = 0.005777
# Epoch  19 Time     18.3 lr = 0.162387 avg loss = 0.005787
# Epoch  19 Time     18.3 lr = 0.162387 avg loss = 0.005776
# Epoch  19 Time     18.4 lr = 0.162387 avg loss = 0.005780
# Epoch  19 Time     18.3 lr = 0.162387 avg loss = 0.005791
# Epoch  19 Time     18.5 lr = 0.162387 avg loss = 0.005784
# Epoch  19 Time     18.4 lr = 0.162387 avg loss = 0.005780
# Epoch  19 Time     18.4 lr = 0.162387 avg loss = 0.005790
# Epoch  19 Time     18.2 lr = 0.162387 avg loss = 0.005796
# Epoch  19 Time     18.4 lr = 0.162387 avg loss = 0.005815
# Epoch  19 Time     18.4 lr = 0.162387 avg loss = 0.005807
# Epoch  19 Time    235.6 lr = 0.162387 avg loss = 0.005813 accuracy = 62.98
# Epoch  20 Time     33.6 lr = 0.157254 avg loss = 0.005719
# Epoch  20 Time     18.4 lr = 0.157254 avg loss = 0.005749
# Epoch  20 Time     18.3 lr = 0.157254 avg loss = 0.005740
# Epoch  20 Time     18.3 lr = 0.157254 avg loss = 0.005759
# Epoch  20 Time     18.3 lr = 0.157254 avg loss = 0.005726
# Epoch  20 Time     18.2 lr = 0.157254 avg loss = 0.005735
# Epoch  20 Time     18.3 lr = 0.157254 avg loss = 0.005729
# Epoch  20 Time     18.4 lr = 0.157254 avg loss = 0.005719
# Epoch  20 Time     18.4 lr = 0.157254 avg loss = 0.005717
# Epoch  20 Time     18.3 lr = 0.157254 avg loss = 0.005716
# Epoch  20 Time     18.5 lr = 0.157254 avg loss = 0.005729
# Epoch  20 Time     18.4 lr = 0.157254 avg loss = 0.005734
# Epoch  20 Time    235.3 lr = 0.157254 avg loss = 0.005727 accuracy = 61.08
# Epoch  21 Time     33.3 lr = 0.151887 avg loss = 0.005583
# Epoch  21 Time     18.3 lr = 0.151887 avg loss = 0.005584
# Epoch  21 Time     18.4 lr = 0.151887 avg loss = 0.005603
# Epoch  21 Time     18.3 lr = 0.151887 avg loss = 0.005588
# Epoch  21 Time     18.4 lr = 0.151887 avg loss = 0.005612
# Epoch  21 Time     18.3 lr = 0.151887 avg loss = 0.005619
# Epoch  21 Time     18.3 lr = 0.151887 avg loss = 0.005648
# Epoch  21 Time     18.4 lr = 0.151887 avg loss = 0.005647
# Epoch  21 Time     18.2 lr = 0.151887 avg loss = 0.005649
# Epoch  21 Time     18.2 lr = 0.151887 avg loss = 0.005652
# Epoch  21 Time     18.2 lr = 0.151887 avg loss = 0.005655
# Epoch  21 Time     18.4 lr = 0.151887 avg loss = 0.005654
# Epoch  21 Time    234.6 lr = 0.151887 avg loss = 0.005657 accuracy = 63.16
# Epoch  22 Time     33.3 lr = 0.146308 avg loss = 0.005458
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005493
# Epoch  22 Time     18.6 lr = 0.146308 avg loss = 0.005498
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005510
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005524
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005531
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005535
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005555
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005561
# Epoch  22 Time     18.3 lr = 0.146308 avg loss = 0.005572
# Epoch  22 Time     18.2 lr = 0.146308 avg loss = 0.005563
# Epoch  22 Time     18.4 lr = 0.146308 avg loss = 0.005565
# Epoch  22 Time    234.8 lr = 0.146308 avg loss = 0.005571 accuracy = 63.02
# Epoch  23 Time     33.4 lr = 0.140538 avg loss = 0.005406
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005348
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005370
# Epoch  23 Time     18.2 lr = 0.140538 avg loss = 0.005395
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005420
# Epoch  23 Time     18.4 lr = 0.140538 avg loss = 0.005442
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005439
# Epoch  23 Time     18.5 lr = 0.140538 avg loss = 0.005435
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005448
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005461
# Epoch  23 Time     18.4 lr = 0.140538 avg loss = 0.005461
# Epoch  23 Time     18.3 lr = 0.140538 avg loss = 0.005459
# Epoch  23 Time    235.2 lr = 0.140538 avg loss = 0.005466 accuracy = 64.36
# Epoch  24 Time     33.4 lr = 0.134602 avg loss = 0.005431
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005356
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005369
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005357
# Epoch  24 Time     18.2 lr = 0.134602 avg loss = 0.005385
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005394
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005393
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005390
# Epoch  24 Time     18.4 lr = 0.134602 avg loss = 0.005399
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005399
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005405
# Epoch  24 Time     18.3 lr = 0.134602 avg loss = 0.005405
# Epoch  24 Time    234.7 lr = 0.134602 avg loss = 0.005408 accuracy = 64.76
# Epoch  25 Time     33.4 lr = 0.128524 avg loss = 0.005203
# Epoch  25 Time     18.4 lr = 0.128524 avg loss = 0.005208
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005238
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005285
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005292
# Epoch  25 Time     18.5 lr = 0.128524 avg loss = 0.005278
# Epoch  25 Time     18.2 lr = 0.128524 avg loss = 0.005296
# Epoch  25 Time     18.2 lr = 0.128524 avg loss = 0.005298
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005290
# Epoch  25 Time     18.4 lr = 0.128524 avg loss = 0.005296
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005306
# Epoch  25 Time     18.3 lr = 0.128524 avg loss = 0.005313
# Epoch  25 Time    234.9 lr = 0.128524 avg loss = 0.005322 accuracy = 63.80
# Epoch  26 Time     33.5 lr = 0.122330 avg loss = 0.005140
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005173
# Epoch  26 Time     18.4 lr = 0.122330 avg loss = 0.005167
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005182
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005196
# Epoch  26 Time     18.4 lr = 0.122330 avg loss = 0.005219
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005219
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005235
# Epoch  26 Time     18.4 lr = 0.122330 avg loss = 0.005242
# Epoch  26 Time     18.4 lr = 0.122330 avg loss = 0.005249
# Epoch  26 Time     18.4 lr = 0.122330 avg loss = 0.005252
# Epoch  26 Time     18.3 lr = 0.122330 avg loss = 0.005260
# Epoch  26 Time    235.3 lr = 0.122330 avg loss = 0.005258 accuracy = 65.82
# Epoch  27 Time     33.5 lr = 0.116044 avg loss = 0.005170
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005154
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005170
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005133
# Epoch  27 Time     18.3 lr = 0.116044 avg loss = 0.005124
# Epoch  27 Time     18.3 lr = 0.116044 avg loss = 0.005127
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005133
# Epoch  27 Time     18.3 lr = 0.116044 avg loss = 0.005143
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005148
# Epoch  27 Time     18.3 lr = 0.116044 avg loss = 0.005152
# Epoch  27 Time     18.4 lr = 0.116044 avg loss = 0.005155
# Epoch  27 Time     18.2 lr = 0.116044 avg loss = 0.005161
# Epoch  27 Time    235.4 lr = 0.116044 avg loss = 0.005163 accuracy = 64.78
# Epoch  28 Time     33.5 lr = 0.109693 avg loss = 0.004992
# Epoch  28 Time     18.3 lr = 0.109693 avg loss = 0.005020
# Epoch  28 Time     18.2 lr = 0.109693 avg loss = 0.005024
# Epoch  28 Time     18.4 lr = 0.109693 avg loss = 0.005032
# Epoch  28 Time     18.4 lr = 0.109693 avg loss = 0.005029
# Epoch  28 Time     18.3 lr = 0.109693 avg loss = 0.005038
# Epoch  28 Time     18.4 lr = 0.109693 avg loss = 0.005052
# Epoch  28 Time     18.2 lr = 0.109693 avg loss = 0.005054
# Epoch  28 Time     18.3 lr = 0.109693 avg loss = 0.005063
# Epoch  28 Time     18.3 lr = 0.109693 avg loss = 0.005072
# Epoch  28 Time     18.4 lr = 0.109693 avg loss = 0.005076
# Epoch  28 Time     18.2 lr = 0.109693 avg loss = 0.005078
# Epoch  28 Time    235.0 lr = 0.109693 avg loss = 0.005077 accuracy = 65.00
# Epoch  29 Time     33.4 lr = 0.103302 avg loss = 0.004894
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004974
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004953
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004956
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004962
# Epoch  29 Time     18.2 lr = 0.103302 avg loss = 0.004973
# Epoch  29 Time     18.4 lr = 0.103302 avg loss = 0.004965
# Epoch  29 Time     18.1 lr = 0.103302 avg loss = 0.004973
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004979
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004983
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.004994
# Epoch  29 Time     18.3 lr = 0.103302 avg loss = 0.005007
# Epoch  29 Time    234.2 lr = 0.103302 avg loss = 0.005012 accuracy = 64.60
# Epoch  30 Time     33.4 lr = 0.096898 avg loss = 0.004958
# Epoch  30 Time     18.1 lr = 0.096898 avg loss = 0.004883
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004890
# Epoch  30 Time     18.5 lr = 0.096898 avg loss = 0.004903
# Epoch  30 Time     18.5 lr = 0.096898 avg loss = 0.004913
# Epoch  30 Time     18.2 lr = 0.096898 avg loss = 0.004909
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004902
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004909
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004915
# Epoch  30 Time     18.4 lr = 0.096898 avg loss = 0.004927
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004926
# Epoch  30 Time     18.3 lr = 0.096898 avg loss = 0.004937
# Epoch  30 Time    234.9 lr = 0.096898 avg loss = 0.004941 accuracy = 66.82
# Epoch  31 Time     33.4 lr = 0.090507 avg loss = 0.004697
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004723
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004747
# Epoch  31 Time     18.2 lr = 0.090507 avg loss = 0.004750
# Epoch  31 Time     18.4 lr = 0.090507 avg loss = 0.004776
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004783
# Epoch  31 Time     18.4 lr = 0.090507 avg loss = 0.004807
# Epoch  31 Time     18.4 lr = 0.090507 avg loss = 0.004809
# Epoch  31 Time     18.2 lr = 0.090507 avg loss = 0.004824
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004832
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004837
# Epoch  31 Time     18.3 lr = 0.090507 avg loss = 0.004843
# Epoch  31 Time    234.7 lr = 0.090507 avg loss = 0.004845 accuracy = 66.08
# Epoch  32 Time     33.3 lr = 0.084156 avg loss = 0.004721
# Epoch  32 Time     18.3 lr = 0.084156 avg loss = 0.004672
# Epoch  32 Time     18.4 lr = 0.084156 avg loss = 0.004691
# Epoch  32 Time     18.4 lr = 0.084156 avg loss = 0.004705
# Epoch  32 Time     18.6 lr = 0.084156 avg loss = 0.004711
# Epoch  32 Time     18.3 lr = 0.084156 avg loss = 0.004699
# Epoch  32 Time     18.2 lr = 0.084156 avg loss = 0.004713
# Epoch  32 Time     18.4 lr = 0.084156 avg loss = 0.004733
# Epoch  32 Time     18.5 lr = 0.084156 avg loss = 0.004740
# Epoch  32 Time     18.6 lr = 0.084156 avg loss = 0.004736
# Epoch  32 Time     18.4 lr = 0.084156 avg loss = 0.004758
# Epoch  32 Time     18.5 lr = 0.084156 avg loss = 0.004773
# Epoch  32 Time    236.1 lr = 0.084156 avg loss = 0.004775 accuracy = 66.26
# Epoch  33 Time     33.4 lr = 0.077870 avg loss = 0.004741
# Epoch  33 Time     18.4 lr = 0.077870 avg loss = 0.004684
# Epoch  33 Time     18.3 lr = 0.077870 avg loss = 0.004688
# Epoch  33 Time     18.3 lr = 0.077870 avg loss = 0.004677
# Epoch  33 Time     18.3 lr = 0.077870 avg loss = 0.004678
# Epoch  33 Time     18.5 lr = 0.077870 avg loss = 0.004678
# Epoch  33 Time     18.3 lr = 0.077870 avg loss = 0.004676
# Epoch  33 Time     18.6 lr = 0.077870 avg loss = 0.004678
# Epoch  33 Time     18.4 lr = 0.077870 avg loss = 0.004678
# Epoch  33 Time     18.4 lr = 0.077870 avg loss = 0.004696
# Epoch  33 Time     18.4 lr = 0.077870 avg loss = 0.004693
# Epoch  33 Time     18.3 lr = 0.077870 avg loss = 0.004701
# Epoch  33 Time    235.6 lr = 0.077870 avg loss = 0.004702 accuracy = 67.44
# Epoch  34 Time     33.5 lr = 0.071676 avg loss = 0.004540
# Epoch  34 Time     18.7 lr = 0.071676 avg loss = 0.004554
# Epoch  34 Time     18.4 lr = 0.071676 avg loss = 0.004556
# Epoch  34 Time     18.4 lr = 0.071676 avg loss = 0.004559
# Epoch  34 Time     18.4 lr = 0.071676 avg loss = 0.004551
# Epoch  34 Time     18.4 lr = 0.071676 avg loss = 0.004563
# Epoch  34 Time     18.3 lr = 0.071676 avg loss = 0.004571
# Epoch  34 Time     18.3 lr = 0.071676 avg loss = 0.004570
# Epoch  34 Time     18.4 lr = 0.071676 avg loss = 0.004569
# Epoch  34 Time     18.3 lr = 0.071676 avg loss = 0.004580
# Epoch  34 Time     18.5 lr = 0.071676 avg loss = 0.004591
# Epoch  34 Time     18.3 lr = 0.071676 avg loss = 0.004602
# Epoch  34 Time    235.9 lr = 0.071676 avg loss = 0.004601 accuracy = 67.44
# Epoch  35 Time     33.4 lr = 0.065598 avg loss = 0.004432
# Epoch  35 Time     18.5 lr = 0.065598 avg loss = 0.004420
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004437
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004442
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004444
# Epoch  35 Time     18.6 lr = 0.065598 avg loss = 0.004470
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004485
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004496
# Epoch  35 Time     18.6 lr = 0.065598 avg loss = 0.004507
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004506
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004509
# Epoch  35 Time     18.4 lr = 0.065598 avg loss = 0.004512
# Epoch  35 Time    236.3 lr = 0.065598 avg loss = 0.004514 accuracy = 68.34
# Epoch  36 Time     33.6 lr = 0.059662 avg loss = 0.004415
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004447
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004445
# Epoch  36 Time     18.4 lr = 0.059662 avg loss = 0.004412
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004394
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004415
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004432
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004444
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004439
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004437
# Epoch  36 Time     18.5 lr = 0.059662 avg loss = 0.004437
# Epoch  36 Time     18.4 lr = 0.059662 avg loss = 0.004436
# Epoch  36 Time    236.7 lr = 0.059662 avg loss = 0.004441 accuracy = 68.46
# Epoch  37 Time     33.4 lr = 0.053892 avg loss = 0.004324
# Epoch  37 Time     18.4 lr = 0.053892 avg loss = 0.004275
# Epoch  37 Time     18.5 lr = 0.053892 avg loss = 0.004299
# Epoch  37 Time     18.4 lr = 0.053892 avg loss = 0.004305
# Epoch  37 Time     18.6 lr = 0.053892 avg loss = 0.004302
# Epoch  37 Time     18.6 lr = 0.053892 avg loss = 0.004306
# Epoch  37 Time     18.6 lr = 0.053892 avg loss = 0.004308
# Epoch  37 Time     18.5 lr = 0.053892 avg loss = 0.004319
# Epoch  37 Time     18.6 lr = 0.053892 avg loss = 0.004321
# Epoch  37 Time     18.6 lr = 0.053892 avg loss = 0.004321
# Epoch  37 Time     18.5 lr = 0.053892 avg loss = 0.004322
# Epoch  37 Time     18.5 lr = 0.053892 avg loss = 0.004331
# Epoch  37 Time    237.3 lr = 0.053892 avg loss = 0.004331 accuracy = 68.36
# Epoch  38 Time     34.0 lr = 0.048313 avg loss = 0.004261
# Epoch  38 Time     18.6 lr = 0.048313 avg loss = 0.004218
# Epoch  38 Time     18.6 lr = 0.048313 avg loss = 0.004207
# Epoch  38 Time     18.6 lr = 0.048313 avg loss = 0.004206
# Epoch  38 Time     18.6 lr = 0.048313 avg loss = 0.004196
# Epoch  38 Time     18.6 lr = 0.048313 avg loss = 0.004202
# Epoch  38 Time     18.5 lr = 0.048313 avg loss = 0.004206
# Epoch  38 Time     18.5 lr = 0.048313 avg loss = 0.004210
# Epoch  38 Time     18.5 lr = 0.048313 avg loss = 0.004226
# Epoch  38 Time     18.4 lr = 0.048313 avg loss = 0.004234
# Epoch  38 Time     18.4 lr = 0.048313 avg loss = 0.004238
# Epoch  38 Time     18.5 lr = 0.048313 avg loss = 0.004250
# Epoch  38 Time    237.8 lr = 0.048313 avg loss = 0.004251 accuracy = 70.16
# Epoch  39 Time     33.7 lr = 0.042946 avg loss = 0.004128
# Epoch  39 Time     18.3 lr = 0.042946 avg loss = 0.004127
# Epoch  39 Time     18.4 lr = 0.042946 avg loss = 0.004139
# Epoch  39 Time     18.4 lr = 0.042946 avg loss = 0.004169
# Epoch  39 Time     18.4 lr = 0.042946 avg loss = 0.004166
# Epoch  39 Time     18.5 lr = 0.042946 avg loss = 0.004167
# Epoch  39 Time     18.3 lr = 0.042946 avg loss = 0.004170
# Epoch  39 Time     18.5 lr = 0.042946 avg loss = 0.004165
# Epoch  39 Time     18.3 lr = 0.042946 avg loss = 0.004169
# Epoch  39 Time     18.3 lr = 0.042946 avg loss = 0.004171
# Epoch  39 Time     18.3 lr = 0.042946 avg loss = 0.004177
# Epoch  39 Time     18.5 lr = 0.042946 avg loss = 0.004182
# Epoch  39 Time    235.8 lr = 0.042946 avg loss = 0.004189 accuracy = 69.58
# Epoch  40 Time     33.6 lr = 0.037813 avg loss = 0.003991
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004003
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004013
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004005
# Epoch  40 Time     18.3 lr = 0.037813 avg loss = 0.004025
# Epoch  40 Time     18.5 lr = 0.037813 avg loss = 0.004041
# Epoch  40 Time     18.5 lr = 0.037813 avg loss = 0.004054
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004053
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004045
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004054
# Epoch  40 Time     18.4 lr = 0.037813 avg loss = 0.004049
# Epoch  40 Time     18.3 lr = 0.037813 avg loss = 0.004055
# Epoch  40 Time    236.0 lr = 0.037813 avg loss = 0.004061 accuracy = 69.04
# Epoch  41 Time     33.6 lr = 0.032937 avg loss = 0.004016
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003969
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003982
# Epoch  41 Time     18.5 lr = 0.032937 avg loss = 0.003997
# Epoch  41 Time     18.3 lr = 0.032937 avg loss = 0.003980
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003972
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003966
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003972
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003980
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003980
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003985
# Epoch  41 Time     18.4 lr = 0.032937 avg loss = 0.003994
# Epoch  41 Time    236.0 lr = 0.032937 avg loss = 0.003998 accuracy = 71.20
# Epoch  42 Time     33.7 lr = 0.028337 avg loss = 0.003949
# Epoch  42 Time     18.4 lr = 0.028337 avg loss = 0.003860
# Epoch  42 Time     18.3 lr = 0.028337 avg loss = 0.003867
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003846
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003852
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003869
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003876
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003875
# Epoch  42 Time     18.5 lr = 0.028337 avg loss = 0.003890
# Epoch  42 Time     18.4 lr = 0.028337 avg loss = 0.003901
# Epoch  42 Time     18.4 lr = 0.028337 avg loss = 0.003903
# Epoch  42 Time     18.4 lr = 0.028337 avg loss = 0.003905
# Epoch  42 Time    236.5 lr = 0.028337 avg loss = 0.003907 accuracy = 70.78
# Epoch  43 Time     33.9 lr = 0.024032 avg loss = 0.003859
# Epoch  43 Time     18.6 lr = 0.024032 avg loss = 0.003843
# Epoch  43 Time     18.5 lr = 0.024032 avg loss = 0.003837
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003809
# Epoch  43 Time     18.6 lr = 0.024032 avg loss = 0.003821
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003806
# Epoch  43 Time     18.5 lr = 0.024032 avg loss = 0.003813
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003817
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003819
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003817
# Epoch  43 Time     18.4 lr = 0.024032 avg loss = 0.003821
# Epoch  43 Time     18.5 lr = 0.024032 avg loss = 0.003824
# Epoch  43 Time    236.7 lr = 0.024032 avg loss = 0.003822 accuracy = 70.66
# Epoch  44 Time     33.6 lr = 0.020039 avg loss = 0.003750
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003707
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003717
# Epoch  44 Time     18.5 lr = 0.020039 avg loss = 0.003730
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003731
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003732
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003721
# Epoch  44 Time     18.5 lr = 0.020039 avg loss = 0.003724
# Epoch  44 Time     18.5 lr = 0.020039 avg loss = 0.003726
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003728
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003738
# Epoch  44 Time     18.4 lr = 0.020039 avg loss = 0.003739
# Epoch  44 Time    236.4 lr = 0.020039 avg loss = 0.003744 accuracy = 71.64
# Epoch  45 Time     33.9 lr = 0.016375 avg loss = 0.003587
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003554
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003571
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003611
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003631
# Epoch  45 Time     18.4 lr = 0.016375 avg loss = 0.003639
# Epoch  45 Time     18.3 lr = 0.016375 avg loss = 0.003635
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003642
# Epoch  45 Time     18.4 lr = 0.016375 avg loss = 0.003652
# Epoch  45 Time     18.4 lr = 0.016375 avg loss = 0.003649
# Epoch  45 Time     18.5 lr = 0.016375 avg loss = 0.003646
# Epoch  45 Time     18.3 lr = 0.016375 avg loss = 0.003644
# Epoch  45 Time    236.5 lr = 0.016375 avg loss = 0.003643 accuracy = 71.48
# Epoch  46 Time     33.5 lr = 0.013055 avg loss = 0.003483
# Epoch  46 Time     18.4 lr = 0.013055 avg loss = 0.003536
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003564
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003554
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003565
# Epoch  46 Time     18.6 lr = 0.013055 avg loss = 0.003576
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003599
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003598
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003599
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003599
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003614
# Epoch  46 Time     18.5 lr = 0.013055 avg loss = 0.003622
# Epoch  46 Time    237.2 lr = 0.013055 avg loss = 0.003616 accuracy = 71.58
# Epoch  47 Time     33.6 lr = 0.010093 avg loss = 0.003491
# Epoch  47 Time     18.5 lr = 0.010093 avg loss = 0.003509
# Epoch  47 Time     18.3 lr = 0.010093 avg loss = 0.003515
# Epoch  47 Time     18.5 lr = 0.010093 avg loss = 0.003544
# Epoch  47 Time     18.4 lr = 0.010093 avg loss = 0.003536
# Epoch  47 Time     18.4 lr = 0.010093 avg loss = 0.003539
# Epoch  47 Time     18.5 lr = 0.010093 avg loss = 0.003535
# Epoch  47 Time     18.5 lr = 0.010093 avg loss = 0.003540
# Epoch  47 Time     18.5 lr = 0.010093 avg loss = 0.003546
# Epoch  47 Time     18.3 lr = 0.010093 avg loss = 0.003544
# Epoch  47 Time     18.4 lr = 0.010093 avg loss = 0.003546
# Epoch  47 Time     18.3 lr = 0.010093 avg loss = 0.003542
# Epoch  47 Time    235.8 lr = 0.010093 avg loss = 0.003542 accuracy = 71.70
# Epoch  48 Time     33.5 lr = 0.007501 avg loss = 0.003444
# Epoch  48 Time     18.6 lr = 0.007501 avg loss = 0.003440
# Epoch  48 Time     18.4 lr = 0.007501 avg loss = 0.003456
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003456
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003460
# Epoch  48 Time     18.3 lr = 0.007501 avg loss = 0.003469
# Epoch  48 Time     18.4 lr = 0.007501 avg loss = 0.003485
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003486
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003486
# Epoch  48 Time     18.4 lr = 0.007501 avg loss = 0.003482
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003479
# Epoch  48 Time     18.5 lr = 0.007501 avg loss = 0.003487
# Epoch  48 Time    236.7 lr = 0.007501 avg loss = 0.003487 accuracy = 72.16
# Epoch  49 Time     33.6 lr = 0.005289 avg loss = 0.003433
# Epoch  49 Time     18.4 lr = 0.005289 avg loss = 0.003416
# Epoch  49 Time     18.3 lr = 0.005289 avg loss = 0.003436
# Epoch  49 Time     18.3 lr = 0.005289 avg loss = 0.003458
# Epoch  49 Time     18.4 lr = 0.005289 avg loss = 0.003454
# Epoch  49 Time     18.2 lr = 0.005289 avg loss = 0.003464
# Epoch  49 Time     18.3 lr = 0.005289 avg loss = 0.003470
# Epoch  49 Time     18.3 lr = 0.005289 avg loss = 0.003457
# Epoch  49 Time     18.6 lr = 0.005289 avg loss = 0.003455
# Epoch  49 Time     18.5 lr = 0.005289 avg loss = 0.003458
# Epoch  49 Time     18.4 lr = 0.005289 avg loss = 0.003461
# Epoch  49 Time     18.6 lr = 0.005289 avg loss = 0.003457
# Epoch  49 Time    236.1 lr = 0.005289 avg loss = 0.003461 accuracy = 71.88
# Epoch  50 Time     33.8 lr = 0.003467 avg loss = 0.003489
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003389
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003404
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003430
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003396
# Epoch  50 Time     18.4 lr = 0.003467 avg loss = 0.003390
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003405
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003415
# Epoch  50 Time     18.4 lr = 0.003467 avg loss = 0.003399
# Epoch  50 Time     18.4 lr = 0.003467 avg loss = 0.003396
# Epoch  50 Time     18.5 lr = 0.003467 avg loss = 0.003402
# Epoch  50 Time     18.6 lr = 0.003467 avg loss = 0.003401
# Epoch  50 Time    237.0 lr = 0.003467 avg loss = 0.003398 accuracy = 72.00
# Epoch  51 Time     33.7 lr = 0.002042 avg loss = 0.003371
# Epoch  51 Time     18.6 lr = 0.002042 avg loss = 0.003311
# Epoch  51 Time     18.5 lr = 0.002042 avg loss = 0.003353
# Epoch  51 Time     18.5 lr = 0.002042 avg loss = 0.003378
# Epoch  51 Time     18.4 lr = 0.002042 avg loss = 0.003378
# Epoch  51 Time     18.5 lr = 0.002042 avg loss = 0.003382
# Epoch  51 Time     18.4 lr = 0.002042 avg loss = 0.003391
# Epoch  51 Time     18.4 lr = 0.002042 avg loss = 0.003400
# Epoch  51 Time     18.5 lr = 0.002042 avg loss = 0.003396
# Epoch  51 Time     18.4 lr = 0.002042 avg loss = 0.003395
# Epoch  51 Time     18.4 lr = 0.002042 avg loss = 0.003390
# Epoch  51 Time     18.5 lr = 0.002042 avg loss = 0.003390
# Epoch  51 Time    236.8 lr = 0.002042 avg loss = 0.003393 accuracy = 71.84
# Epoch  52 Time     33.6 lr = 0.001020 avg loss = 0.003371
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003348
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003351
# Epoch  52 Time     18.6 lr = 0.001020 avg loss = 0.003349
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003370
# Epoch  52 Time     18.5 lr = 0.001020 avg loss = 0.003352
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003354
# Epoch  52 Time     18.5 lr = 0.001020 avg loss = 0.003353
# Epoch  52 Time     18.5 lr = 0.001020 avg loss = 0.003357
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003366
# Epoch  52 Time     18.5 lr = 0.001020 avg loss = 0.003361
# Epoch  52 Time     18.4 lr = 0.001020 avg loss = 0.003364
# Epoch  52 Time    236.7 lr = 0.001020 avg loss = 0.003352 accuracy = 72.08
# Epoch  53 Time     33.7 lr = 0.000405 avg loss = 0.003340
# Epoch  53 Time     18.4 lr = 0.000405 avg loss = 0.003311
# Epoch  53 Time     18.5 lr = 0.000405 avg loss = 0.003329
# Epoch  53 Time     18.4 lr = 0.000405 avg loss = 0.003345
# Epoch  53 Time     18.4 lr = 0.000405 avg loss = 0.003351
# Epoch  53 Time     18.5 lr = 0.000405 avg loss = 0.003354
# Epoch  53 Time     18.5 lr = 0.000405 avg loss = 0.003353
# Epoch  53 Time     18.5 lr = 0.000405 avg loss = 0.003359
# Epoch  53 Time     18.4 lr = 0.000405 avg loss = 0.003352
# Epoch  53 Time     18.4 lr = 0.000405 avg loss = 0.003360
# Epoch  53 Time     18.3 lr = 0.000405 avg loss = 0.003360
# Epoch  53 Time     18.3 lr = 0.000405 avg loss = 0.003357
# Epoch  53 Time    236.3 lr = 0.000405 avg loss = 0.003360 accuracy = 72.20
# Epoch  54 Time     33.5 lr = 0.000200 avg loss = 0.003423
# Epoch  54 Time     18.2 lr = 0.000200 avg loss = 0.003379
# Epoch  54 Time     18.3 lr = 0.000200 avg loss = 0.003370
# Epoch  54 Time     18.3 lr = 0.000200 avg loss = 0.003361
# Epoch  54 Time     18.2 lr = 0.000200 avg loss = 0.003344
# Epoch  54 Time     18.4 lr = 0.000200 avg loss = 0.003362
# Epoch  54 Time     18.6 lr = 0.000200 avg loss = 0.003350
# Epoch  54 Time     18.8 lr = 0.000200 avg loss = 0.003344
# Epoch  54 Time     18.5 lr = 0.000200 avg loss = 0.003340
# Epoch  54 Time     18.5 lr = 0.000200 avg loss = 0.003341
# Epoch  54 Time     18.5 lr = 0.000200 avg loss = 0.003339
# Epoch  54 Time     18.3 lr = 0.000200 avg loss = 0.003345
# Epoch  54 Time    236.1 lr = 0.000200 avg loss = 0.003343 accuracy = 72.26
#    3. Submit eff.py, eff_se.py (if done) and eff_se_cond.py (if done) via
#       eLearning (no zip files, no Jupyter / iPython notebooks, ...) with this
#       comment block at the top and all code from the IMPORT comment block to
#       the end; so if you implement all 3, you will submit 3 Python files
#
# HELP
#
#    1. If you're looking for a reference for block and network design, see
#       https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Tests/202008/xNNs_Project_002_Networks.py
#       which implemented a RegNetX style block and network; while the block and
#       network is different, that code should help with thinking about how to
#       organize this code
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

# torch
import torch
import torch.nn       as     nn
import torch.optim    as     optim
from   torch.autograd import Function

# torch utils
import torchvision
import torchvision.transforms as transforms

# additional libraries
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt

################################################################################
#
# PARAMETERS
#
################################################################################

# data
DATA_DIR_1        = 'data'
DATA_DIR_2        = 'data/imagenet64'
DATA_DIR_TRAIN    = 'data/imagenet64/train'
DATA_DIR_TEST     = 'data/imagenet64/val'
DATA_FILE_TRAIN_1 = 'Train1.zip'
DATA_FILE_TRAIN_2 = 'Train2.zip'
DATA_FILE_TRAIN_3 = 'Train3.zip'
DATA_FILE_TRAIN_4 = 'Train4.zip'
DATA_FILE_TRAIN_5 = 'Train5.zip'
DATA_FILE_TEST_1  = 'Val1.zip'
DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'
DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'
DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'
DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'
DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'
DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'
DATA_BATCH_SIZE   = 256
DATA_NUM_WORKERS  = 4
DATA_NUM_CHANNELS = 3
DATA_NUM_CLASSES  = 100
DATA_RESIZE       = 64
DATA_CROP         = 56
DATA_MEAN         = (0.485, 0.456, 0.406)
DATA_STD_DEV      = (0.229, 0.224, 0.225)

# model
MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation
MODEL_LEVEL_2_BLOCKS   = 1
MODEL_LEVEL_3_BLOCKS   = 2
MODEL_LEVEL_4_BLOCKS   = 3
MODEL_LEVEL_5_BLOCKS   = 4
MODEL_LEVEL_1_CHANNELS = 16
MODEL_LEVEL_2_CHANNELS = 24
MODEL_LEVEL_3_CHANNELS = 40
MODEL_LEVEL_4_CHANNELS = 80
MODEL_LEVEL_5_CHANNELS = 160
MODEL_LEVEL_6_CHANNELS = 320
MODEL_LEVEL_7_CHANNELS = 1280

# training
TRAIN_LR_MAX              = 0.2
TRAIN_LR_INIT_SCALE       = 0.01
TRAIN_LR_FINAL_SCALE      = 0.001
TRAIN_LR_INIT_EPOCHS      = 5
TRAIN_LR_FINAL_EPOCHS     = 50 # 100
TRAIN_NUM_EPOCHS          = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS
TRAIN_LR_INIT             = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE
TRAIN_LR_FINAL            = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE
TRAIN_INTRA_EPOCH_DISPLAY = 10000

# file
FILE_NAME_CHECK      = 'EffNetStyleCheck.pt'
FILE_NAME_BEST       = 'EffNetStyleBest.pt'
FILE_SAVE            = True
FILE_LOAD            = False
FILE_EXTEND_TRAINING = False
FILE_NEW_OPTIMIZER   = False

################################################################################
#
# DATA
#
################################################################################

# create a local directory structure for data storage
if (os.path.exists(DATA_DIR_1) == False):
    os.mkdir(DATA_DIR_1)
if (os.path.exists(DATA_DIR_2) == False):
    os.mkdir(DATA_DIR_2)
if (os.path.exists(DATA_DIR_TRAIN) == False):
    os.mkdir(DATA_DIR_TRAIN)
if (os.path.exists(DATA_DIR_TEST) == False):
    os.mkdir(DATA_DIR_TEST)

# download data
if (os.path.exists(DATA_FILE_TRAIN_1) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)
if (os.path.exists(DATA_FILE_TRAIN_2) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)
if (os.path.exists(DATA_FILE_TRAIN_3) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)
if (os.path.exists(DATA_FILE_TRAIN_4) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)
if (os.path.exists(DATA_FILE_TRAIN_5) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)
if (os.path.exists(DATA_FILE_TEST_1) == False):
    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)

# extract data
with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TEST)

# transforms
transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])
transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])

# data sets
dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)
dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)

# data loader
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True)
dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False)

################################################################################
#
# NETWORK BUILDING BLOCK
#
################################################################################

# inverted residual block
class InvResBlock(nn.Module):

    # initialization
    def __init__(self, Ni, Ne, No, F, S):

        # parent initialization
        super(InvResBlock, self).__init__()

        # create all of the operators for the inverted residual block in fig 2a
        # of the paper; note that parameter names were chosen to match the paper

    #identity:
        if((Ni == No) and S==1):
          self.id =True
       # self.conv0 = nn.Conv2d(Ni,No,(3,3),stride=S,bias=False)
        #self.bn0 = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        else:
        #residual
          self.id = False

        P = np.floor(F/2).astype(int)
        self.conv1 = nn.Conv2d(Ni, Ne, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros')
        self.bn1   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(Ne, Ne, kernel_size=F, stride=S, padding=P, dilation=(1, 1), groups=Ne, bias=False, padding_mode='zeros')
        self.bn2   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu2 = nn.ReLU()
        self.conv3 = nn.Conv2d(Ne, No, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros')
        self.bn3   = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

        #sum
        #self.relu0 = nn.ReLU()
    # forward path
    def forward(self, x):

        # identity
        if (self.id == True):
            id = x

        # residual
        res = self.conv1(x)
        res = self.bn1(res)
        res = self.relu1(res)
        res = self.conv2(res)
        res = self.bn2(res)
        res = self.relu2(res)
        res = self.conv3(res)
        res = self.bn3(res)

        # sum
        if(self.id==True):
          y = id + res
        else:
          y=res
        #y = self.relu0(y)

        # return
        return y
################################################################################
#
# NETWORK
#
################################################################################

# define
class Model(nn.Module):

    # initialization
    # add necessary parameters to the init function to create the model defined
    # in table 1 of the paper
    # initialization
    def __init__(self,
                 data_num_channels,
                 model_level_1_blocks, model_level_1_channels,
                 model_level_2_blocks, model_level_2_channels,
                 model_level_3_blocks, model_level_3_channels, 
                 model_level_4_blocks, model_level_4_channels, 
                 model_level_5_blocks, model_level_5_channels,
                 model_level_6_channels,model_level_7_channels, 
                 data_num_classes):

        # parent initialization
        super(Model, self).__init__()

        # create all of the operators for the network defined in table 1 of the
        # paper using a combination of Python, standard PyTorch operators and
        # the previously defined InvResBlock class
        # <TO DO: your code goes here>

        # stride
        stride1 = 1 
        stride2 = 2 
        stride3 = 2
        stride4 = 2
        stride5 = 1

        # encoder level 1 - Tail
        self.tail = nn.ModuleList()
        self.tail.append(nn.Conv2d(data_num_channels, model_level_1_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros'))
        self.tail.append(nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.tail.append(nn.ReLU())

 

        # encoder level 2 Body 1
        self.enc_1 = nn.ModuleList()
        for n in range(model_level_1_blocks-1):
            self.enc_1.append(InvResBlock(model_level_1_channels, model_level_1_channels * 4, model_level_1_channels, 3, 1))
        self.enc_1.append(InvResBlock(model_level_1_channels, model_level_1_channels * 4, model_level_2_channels, 3, stride1))
        #print("phase 1")

        # encoder level 3 Body 2
        self.enc_2 = nn.ModuleList()
        for n in range(model_level_2_blocks-1):
            self.enc_2.append(InvResBlock(model_level_2_channels, model_level_2_channels * 4, model_level_2_channels, 3, 1))
        self.enc_2.append(InvResBlock(model_level_2_channels, model_level_2_channels * 4, model_level_3_channels, 3, stride2))
        #print("phase 2")

        # encoder level 4 Body 3
        self.enc_3 = nn.ModuleList()
        for n in range(model_level_3_blocks-1):
            self.enc_3.append(InvResBlock(model_level_3_channels, model_level_3_channels * 4, model_level_3_channels, 3, 1))        
        self.enc_3.append(InvResBlock(model_level_3_channels, model_level_3_channels * 4, model_level_4_channels, 3, stride3))

        
         # encoder level 5  Body 4
        self.enc_4 = nn.ModuleList()
        for n in range(model_level_4_blocks-1):
            self.enc_4.append(InvResBlock(model_level_4_channels, model_level_4_channels * 4, model_level_4_channels, 3, 1))
        self.enc_4.append(InvResBlock(model_level_4_channels, model_level_4_channels * 4, model_level_5_channels, 3, stride4))


        # encoder level 6  Body 5
        self.enc_5 = nn.ModuleList()
        for n in range(model_level_5_blocks-1):
            self.enc_5.append(InvResBlock(model_level_5_channels, model_level_5_channels * 4, model_level_5_channels, 3, 1))
        self.enc_5.append(InvResBlock(model_level_5_channels, model_level_5_channels * 4, model_level_6_channels, 3, stride5))

        


        # decoder
        self.dec = nn.ModuleList()
        self.dec.append(nn.Conv2d(model_level_6_channels, model_level_7_channels, 1, stride=1, groups=1, bias=False))
        self.dec.append(nn.BatchNorm2d(model_level_7_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.dec.append(nn.ReLU())
        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))
        self.dec.append(nn.Flatten())
        self.dec.append(nn.Linear(model_level_7_channels, data_num_classes, bias=True))

    # forward path
    def forward(self, x):
        #Tail part:
        for layer in self.tail:
          x=layer(x)
        # encoder level 1
        for layer in self.enc_1:
            x = layer(x)

        # encoder level 2
        for layer in self.enc_2:
            x = layer(x)

        # encoder level 3
        for layer in self.enc_3:
            x = layer(x)

        # encoder level 4
        for layer in self.enc_4:
            x = layer(x)

        # encoder level 5
        for layer in self.enc_5:
            x = layer(x)

        # decoder
        for layer in self.dec:
            x = layer(x)

        # map input x to output y for the network defined in table 1 of the
        # paper via connecting the operators defined in the initialization
        # and return output y
        # <TO DO: your code goes here>
        y=x
        # return
        return y

# create
# add necessary parameters to the init function to create the model defined
# in table 1 of the paper
model = Model(DATA_NUM_CHANNELS,
              MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_1_CHANNELS, 
              MODEL_LEVEL_2_BLOCKS, MODEL_LEVEL_2_CHANNELS, 
              MODEL_LEVEL_3_BLOCKS, MODEL_LEVEL_3_CHANNELS, 
              MODEL_LEVEL_4_BLOCKS, MODEL_LEVEL_4_CHANNELS, 
              MODEL_LEVEL_5_BLOCKS, MODEL_LEVEL_5_CHANNELS,
              MODEL_LEVEL_6_CHANNELS,MODEL_LEVEL_7_CHANNELS,
              DATA_NUM_CLASSES) # <TO DO: your code goes here> inside the parenthesis

# enable data parallelization for multi GPU systems
if (torch.cuda.device_count() > 1):
    model = nn.DataParallel(model)
print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)

################################################################################
#
# ERROR AND OPTIMIZER
#
################################################################################

# error (softmax cross entropy)
criterion = nn.CrossEntropyLoss()

# learning rate schedule
def lr_schedule(epoch):

    # linear warmup followed by 1/2 wave cosine decay
    if epoch < TRAIN_LR_INIT_EPOCHS:
        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT
    else:
        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))

    return lr

# optimizer
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, dampening=0.0, weight_decay=5e-5, nesterov=True)

################################################################################
#
# TRAINING
#
################################################################################

# start epoch
start_epoch = 0

# specify the device as the GPU if present with fallback to the CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# transfer the network to the device
model.to(device)

# load the last checkpoint
if (FILE_LOAD == True):
    checkpoint = torch.load(FILE_NAME_CHECK)
    model.load_state_dict(checkpoint['model_state_dict'])
    if (FILE_NEW_OPTIMIZER == False):
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if (FILE_EXTEND_TRAINING == False):
        start_epoch = checkpoint['epoch'] + 1

# initialize the epoch
accuracy_best      = 0
start_time_display = time.time()
start_time_epoch   = time.time()

# cycle through the epochs
for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):

    # initialize epoch training
    model.train()
    training_loss = 0.0
    num_batches   = 0
    num_display   = 0

    # set the learning rate for the epoch
    for g in optimizer.param_groups:
        g['lr'] = lr_schedule(epoch)

    # cycle through the training data set
    for data in dataloader_train:

        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward pass, loss, backward pass and weight update
        outputs = model(inputs)
        loss    = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # update statistics
        training_loss = training_loss + loss.item()
        num_batches   = num_batches + 1
        num_display   = num_display + DATA_BATCH_SIZE

        # display intra epoch results
        if (num_display > TRAIN_INTRA_EPOCH_DISPLAY):
            num_display          = 0
            elapsed_time_display = time.time() - start_time_display
            start_time_display   = time.time()
            print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f}'.format(epoch, elapsed_time_display, lr_schedule(epoch), (training_loss / num_batches) / DATA_BATCH_SIZE), flush=True)

    # initialize epoch testing
    model.eval()
    test_correct = 0
    test_total   = 0

    # no weight update / no gradient needed
    with torch.no_grad():

        # cycle through the testing data set
        for data in dataloader_test:

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            # update test set statistics
            test_total   = test_total + labels.size(0)
            test_correct = test_correct + (predicted == labels).sum().item()

    # epoch statistics
    elapsed_time_epoch = time.time() - start_time_epoch
    start_time_epoch   = time.time()
    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)), flush=True)

    # save a checkpoint
    if (FILE_SAVE == True):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_CHECK)

    # save the best model
    accuracy_epoch = 100.0 * test_correct / test_total
    if ((FILE_SAVE == True) and (accuracy_epoch >= accuracy_best)):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_BEST)

