# -*- coding: utf-8 -*-
"""xNN_invertedSE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JZlEjUq1aQEzydxqeMWgu95-SK28zLco
"""

################################################################################
#
# LOGISTICS
#
#    <TO DO: first and last name as in eLearning>
#     An Yu Liu
#    <TO DO: UTD ID>
#     AXL180015
#    <TO DO: this comment block is included in each file that is submitted:
#    eff.py, eff_se.py (if done) and eff_se_cond.py (if done)>
#    eff.py, eff_se.py
# FILE
#
#    eff.py | eff_se.py | eff_se_cond.py
#
# DESCRIPTION
#
#    Grade = eff.py grade (max 90) + eff_se.py grade (max 10) + eff_se_cond.py grade (max 10)
#
#    A PyTorch implementation of the network described in section 3 of
#    xNNs_Project_002_NetworksPaper.doc/pdf trained in Google Colaboratory using
#    a GPU instance (Runtime - Change runtime type - Hardware accelerator - GPU)
#
# INSTRUCTIONS
#
#    1. a. eff.py
#
#          Complete all <TO DO: ...> code portions of this file to design and
#          train the network in table 1 of the paper with the standard inverted
#          residual building block in fig 2a and report the results
#
#    1. b. eff_se.py
#
#          Starting from eff.py, create a SE block per fig 3 in the paper and
#          add it to the inverted residual block per fig 2b in the paper to
#          create a SE enhanced building block; design and train the network
#          in table 1 with the SE enhanced building block and report the same
#          results as in the standard building block case
#
#    1. c. eff_se_cond.py
#
#          Starting from eff_se.py, create a conditional conv operation per
#          fig 4 in the paper and replace the building block convolutions with
#          the conditional convolution operation to create a SE and conditional
#          convolution enhanced building block per fig 2c in the paper; design
#          and train the network in table 1 with the SE and conditional
#          convolution enhanced building block and report the same results as in
#          the standard building block case; it may be required to reduce the
#          batch size in this case if there is an out of memory error
#
#    2. Cut and paste the text output generated during training showing the per
#       epoch statistics (for all networks trained: standard, SE enhanced and
#       SE and conditional convolution enhanced)
#
# Epoch   0 Time     20.5 lr = 0.002000 avg loss = 0.017977
# Epoch   0 Time     20.4 lr = 0.002000 avg loss = 0.017891
# Epoch   0 Time     20.5 lr = 0.002000 avg loss = 0.017820
# Epoch   0 Time     20.8 lr = 0.002000 avg loss = 0.017746
# Epoch   0 Time     20.9 lr = 0.002000 avg loss = 0.017643
# Epoch   0 Time     21.0 lr = 0.002000 avg loss = 0.017524
# Epoch   0 Time     21.0 lr = 0.002000 avg loss = 0.017388
# Epoch   0 Time     20.9 lr = 0.002000 avg loss = 0.017255
# Epoch   0 Time     20.9 lr = 0.002000 avg loss = 0.017128
# Epoch   0 Time     21.1 lr = 0.002000 avg loss = 0.016997
# Epoch   0 Time     20.9 lr = 0.002000 avg loss = 0.016875
# Epoch   0 Time     20.8 lr = 0.002000 avg loss = 0.016758
# Epoch   0 Time    266.7 lr = 0.002000 avg loss = 0.016687 accuracy =  9.20
# Epoch   1 Time     38.0 lr = 0.041600 avg loss = 0.015290
# Epoch   1 Time     20.6 lr = 0.041600 avg loss = 0.014959
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.014631
# Epoch   1 Time     20.6 lr = 0.041600 avg loss = 0.014383
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.014144
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.013927
# Epoch   1 Time     20.6 lr = 0.041600 avg loss = 0.013745
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.013569
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.013390
# Epoch   1 Time     20.8 lr = 0.041600 avg loss = 0.013220
# Epoch   1 Time     20.7 lr = 0.041600 avg loss = 0.013067
# Epoch   1 Time     20.6 lr = 0.041600 avg loss = 0.012920
# Epoch   1 Time    265.2 lr = 0.041600 avg loss = 0.012827 accuracy = 29.30
# Epoch   2 Time     37.7 lr = 0.081200 avg loss = 0.011606
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.011591
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.011434
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.011320
# Epoch   2 Time     20.5 lr = 0.081200 avg loss = 0.011234
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.011133
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.011025
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.010940
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.010846
# Epoch   2 Time     20.7 lr = 0.081200 avg loss = 0.010772
# Epoch   2 Time     20.5 lr = 0.081200 avg loss = 0.010687
# Epoch   2 Time     20.6 lr = 0.081200 avg loss = 0.010612
# Epoch   2 Time    264.1 lr = 0.081200 avg loss = 0.010574 accuracy = 37.66
# Epoch   3 Time     37.7 lr = 0.120800 avg loss = 0.009971
# Epoch   3 Time     20.7 lr = 0.120800 avg loss = 0.009976
# Epoch   3 Time     20.7 lr = 0.120800 avg loss = 0.009963
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009906
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009877
# Epoch   3 Time     20.6 lr = 0.120800 avg loss = 0.009816
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009774
# Epoch   3 Time     20.6 lr = 0.120800 avg loss = 0.009715
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009650
# Epoch   3 Time     20.4 lr = 0.120800 avg loss = 0.009584
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009552
# Epoch   3 Time     20.5 lr = 0.120800 avg loss = 0.009509
# Epoch   3 Time    263.3 lr = 0.120800 avg loss = 0.009478 accuracy = 43.18
# Epoch   4 Time     37.0 lr = 0.160400 avg loss = 0.009161
# Epoch   4 Time     20.6 lr = 0.160400 avg loss = 0.009113
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.009030
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.009016
# Epoch   4 Time     20.6 lr = 0.160400 avg loss = 0.009023
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.008970
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.008927
# Epoch   4 Time     20.4 lr = 0.160400 avg loss = 0.008894
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.008856
# Epoch   4 Time     20.5 lr = 0.160400 avg loss = 0.008816
# Epoch   4 Time     20.6 lr = 0.160400 avg loss = 0.008782
# Epoch   4 Time     20.3 lr = 0.160400 avg loss = 0.008757
# Epoch   4 Time    262.5 lr = 0.160400 avg loss = 0.008738 accuracy = 44.10
# Epoch   5 Time     37.2 lr = 0.200000 avg loss = 0.008324
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008331
# Epoch   5 Time     20.4 lr = 0.200000 avg loss = 0.008360
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008361
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008363
# Epoch   5 Time     20.6 lr = 0.200000 avg loss = 0.008334
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008331
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008306
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008294
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008277
# Epoch   5 Time     20.5 lr = 0.200000 avg loss = 0.008267
# Epoch   5 Time     20.4 lr = 0.200000 avg loss = 0.008239
# Epoch   5 Time    262.5 lr = 0.200000 avg loss = 0.008222 accuracy = 48.34
# Epoch   6 Time     37.1 lr = 0.199795 avg loss = 0.007757
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007777
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007772
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007770
# Epoch   6 Time     20.3 lr = 0.199795 avg loss = 0.007764
# Epoch   6 Time     20.6 lr = 0.199795 avg loss = 0.007745
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007745
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007735
# Epoch   6 Time     20.4 lr = 0.199795 avg loss = 0.007721
# Epoch   6 Time     20.4 lr = 0.199795 avg loss = 0.007711
# Epoch   6 Time     20.5 lr = 0.199795 avg loss = 0.007689
# Epoch   6 Time     20.4 lr = 0.199795 avg loss = 0.007660
# Epoch   6 Time    262.2 lr = 0.199795 avg loss = 0.007653 accuracy = 52.56
# Epoch   7 Time     37.1 lr = 0.199180 avg loss = 0.007346
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007352
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007325
# Epoch   7 Time     20.2 lr = 0.199180 avg loss = 0.007328
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007314
# Epoch   7 Time     20.3 lr = 0.199180 avg loss = 0.007302
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007298
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007297
# Epoch   7 Time     20.2 lr = 0.199180 avg loss = 0.007293
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007282
# Epoch   7 Time     20.4 lr = 0.199180 avg loss = 0.007282
# Epoch   7 Time     20.3 lr = 0.199180 avg loss = 0.007275
# Epoch   7 Time    260.9 lr = 0.199180 avg loss = 0.007276 accuracy = 53.22
# Epoch   8 Time     36.9 lr = 0.198158 avg loss = 0.007049
# Epoch   8 Time     20.4 lr = 0.198158 avg loss = 0.007051
# Epoch   8 Time     20.4 lr = 0.198158 avg loss = 0.007048
# Epoch   8 Time     20.5 lr = 0.198158 avg loss = 0.007035
# Epoch   8 Time     20.9 lr = 0.198158 avg loss = 0.007005
# Epoch   8 Time     20.9 lr = 0.198158 avg loss = 0.006996
# Epoch   8 Time     20.8 lr = 0.198158 avg loss = 0.006987
# Epoch   8 Time     20.5 lr = 0.198158 avg loss = 0.006992
# Epoch   8 Time     20.5 lr = 0.198158 avg loss = 0.006996
# Epoch   8 Time     20.5 lr = 0.198158 avg loss = 0.006992
# Epoch   8 Time     20.5 lr = 0.198158 avg loss = 0.006995
# Epoch   8 Time     20.4 lr = 0.198158 avg loss = 0.006976
# Epoch   8 Time    263.4 lr = 0.198158 avg loss = 0.006976 accuracy = 55.86
# Epoch   9 Time     37.2 lr = 0.196733 avg loss = 0.006765
# Epoch   9 Time     20.5 lr = 0.196733 avg loss = 0.006695
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006686
# Epoch   9 Time     20.5 lr = 0.196733 avg loss = 0.006688
# Epoch   9 Time     20.5 lr = 0.196733 avg loss = 0.006676
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006703
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006704
# Epoch   9 Time     20.3 lr = 0.196733 avg loss = 0.006706
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006694
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006696
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006700
# Epoch   9 Time     20.4 lr = 0.196733 avg loss = 0.006700
# Epoch   9 Time    261.6 lr = 0.196733 avg loss = 0.006704 accuracy = 55.98
# Epoch  10 Time     37.1 lr = 0.194911 avg loss = 0.006436
# Epoch  10 Time     20.5 lr = 0.194911 avg loss = 0.006468
# Epoch  10 Time     20.5 lr = 0.194911 avg loss = 0.006492
# Epoch  10 Time     20.5 lr = 0.194911 avg loss = 0.006507
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006507
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006524
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006541
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006543
# Epoch  10 Time     20.3 lr = 0.194911 avg loss = 0.006528
# Epoch  10 Time     20.5 lr = 0.194911 avg loss = 0.006526
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006519
# Epoch  10 Time     20.4 lr = 0.194911 avg loss = 0.006515
# Epoch  10 Time    261.7 lr = 0.194911 avg loss = 0.006515 accuracy = 57.32
# Epoch  11 Time     36.9 lr = 0.192699 avg loss = 0.006292
# Epoch  11 Time     20.4 lr = 0.192699 avg loss = 0.006273
# Epoch  11 Time     20.4 lr = 0.192699 avg loss = 0.006262
# Epoch  11 Time     20.5 lr = 0.192699 avg loss = 0.006247
# Epoch  11 Time     20.5 lr = 0.192699 avg loss = 0.006283
# Epoch  11 Time     20.6 lr = 0.192699 avg loss = 0.006310
# Epoch  11 Time     20.3 lr = 0.192699 avg loss = 0.006331
# Epoch  11 Time     20.4 lr = 0.192699 avg loss = 0.006330
# Epoch  11 Time     20.5 lr = 0.192699 avg loss = 0.006341
# Epoch  11 Time     20.3 lr = 0.192699 avg loss = 0.006341
# Epoch  11 Time     20.4 lr = 0.192699 avg loss = 0.006334
# Epoch  11 Time     20.5 lr = 0.192699 avg loss = 0.006333
# Epoch  11 Time    261.8 lr = 0.192699 avg loss = 0.006330 accuracy = 60.38
# Epoch  12 Time     37.1 lr = 0.190107 avg loss = 0.006093
# Epoch  12 Time     20.5 lr = 0.190107 avg loss = 0.006144
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006142
# Epoch  12 Time     20.5 lr = 0.190107 avg loss = 0.006173
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006198
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006188
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006181
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006165
# Epoch  12 Time     20.4 lr = 0.190107 avg loss = 0.006171
# Epoch  12 Time     20.5 lr = 0.190107 avg loss = 0.006182
# Epoch  12 Time     20.5 lr = 0.190107 avg loss = 0.006182
# Epoch  12 Time     20.5 lr = 0.190107 avg loss = 0.006173
# Epoch  12 Time    262.0 lr = 0.190107 avg loss = 0.006176 accuracy = 59.34
# Epoch  13 Time     37.2 lr = 0.187145 avg loss = 0.005926
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.005998
# Epoch  13 Time     20.5 lr = 0.187145 avg loss = 0.006028
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.006046
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.006035
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.006029
# Epoch  13 Time     20.6 lr = 0.187145 avg loss = 0.006035
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.006037
# Epoch  13 Time     20.5 lr = 0.187145 avg loss = 0.006046
# Epoch  13 Time     20.5 lr = 0.187145 avg loss = 0.006051
# Epoch  13 Time     20.3 lr = 0.187145 avg loss = 0.006045
# Epoch  13 Time     20.4 lr = 0.187145 avg loss = 0.006050
# Epoch  13 Time    262.0 lr = 0.187145 avg loss = 0.006053 accuracy = 62.18
# Epoch  14 Time     37.1 lr = 0.183825 avg loss = 0.005824
# Epoch  14 Time     20.3 lr = 0.183825 avg loss = 0.005878
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005849
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005868
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005862
# Epoch  14 Time     20.5 lr = 0.183825 avg loss = 0.005866
# Epoch  14 Time     20.6 lr = 0.183825 avg loss = 0.005869
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005887
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005887
# Epoch  14 Time     20.4 lr = 0.183825 avg loss = 0.005897
# Epoch  14 Time     20.3 lr = 0.183825 avg loss = 0.005900
# Epoch  14 Time     20.3 lr = 0.183825 avg loss = 0.005908
# Epoch  14 Time    261.3 lr = 0.183825 avg loss = 0.005906 accuracy = 61.98
# Epoch  15 Time     36.7 lr = 0.180161 avg loss = 0.005669
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005710
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005750
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005771
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005787
# Epoch  15 Time     20.4 lr = 0.180161 avg loss = 0.005774
# Epoch  15 Time     20.4 lr = 0.180161 avg loss = 0.005781
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005782
# Epoch  15 Time     20.3 lr = 0.180161 avg loss = 0.005789
# Epoch  15 Time     20.2 lr = 0.180161 avg loss = 0.005793
# Epoch  15 Time     20.4 lr = 0.180161 avg loss = 0.005787
# Epoch  15 Time     20.4 lr = 0.180161 avg loss = 0.005787
# Epoch  15 Time    260.6 lr = 0.180161 avg loss = 0.005792 accuracy = 63.66
# Epoch  16 Time     37.3 lr = 0.176168 avg loss = 0.005498
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005589
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005619
# Epoch  16 Time     20.2 lr = 0.176168 avg loss = 0.005650
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005674
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005660
# Epoch  16 Time     20.4 lr = 0.176168 avg loss = 0.005666
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005675
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005677
# Epoch  16 Time     20.4 lr = 0.176168 avg loss = 0.005681
# Epoch  16 Time     20.3 lr = 0.176168 avg loss = 0.005662
# Epoch  16 Time     20.1 lr = 0.176168 avg loss = 0.005681
# Epoch  16 Time    260.4 lr = 0.176168 avg loss = 0.005684 accuracy = 64.14
# Epoch  17 Time     36.8 lr = 0.171863 avg loss = 0.005425
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005475
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005509
# Epoch  17 Time     20.2 lr = 0.171863 avg loss = 0.005524
# Epoch  17 Time     20.2 lr = 0.171863 avg loss = 0.005528
# Epoch  17 Time     20.4 lr = 0.171863 avg loss = 0.005547
# Epoch  17 Time     20.2 lr = 0.171863 avg loss = 0.005568
# Epoch  17 Time     20.2 lr = 0.171863 avg loss = 0.005565
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005578
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005598
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005597
# Epoch  17 Time     20.3 lr = 0.171863 avg loss = 0.005593
# Epoch  17 Time    259.6 lr = 0.171863 avg loss = 0.005598 accuracy = 60.56
# Epoch  18 Time     36.9 lr = 0.167263 avg loss = 0.005307
# Epoch  18 Time     20.6 lr = 0.167263 avg loss = 0.005376
# Epoch  18 Time     20.3 lr = 0.167263 avg loss = 0.005420
# Epoch  18 Time     20.4 lr = 0.167263 avg loss = 0.005441
# Epoch  18 Time     20.5 lr = 0.167263 avg loss = 0.005442
# Epoch  18 Time     20.5 lr = 0.167263 avg loss = 0.005425
# Epoch  18 Time     20.5 lr = 0.167263 avg loss = 0.005441
# Epoch  18 Time     20.5 lr = 0.167263 avg loss = 0.005447
# Epoch  18 Time     20.4 lr = 0.167263 avg loss = 0.005449
# Epoch  18 Time     20.3 lr = 0.167263 avg loss = 0.005462
# Epoch  18 Time     20.5 lr = 0.167263 avg loss = 0.005474
# Epoch  18 Time     20.4 lr = 0.167263 avg loss = 0.005480
# Epoch  18 Time    262.2 lr = 0.167263 avg loss = 0.005483 accuracy = 66.22
# Epoch  19 Time     37.2 lr = 0.162387 avg loss = 0.005308
# Epoch  19 Time     20.7 lr = 0.162387 avg loss = 0.005344
# Epoch  19 Time     20.6 lr = 0.162387 avg loss = 0.005362
# Epoch  19 Time     20.5 lr = 0.162387 avg loss = 0.005368
# Epoch  19 Time     20.6 lr = 0.162387 avg loss = 0.005383
# Epoch  19 Time     20.5 lr = 0.162387 avg loss = 0.005398
# Epoch  19 Time     20.7 lr = 0.162387 avg loss = 0.005392
# Epoch  19 Time     20.8 lr = 0.162387 avg loss = 0.005398
# Epoch  19 Time     20.9 lr = 0.162387 avg loss = 0.005398
# Epoch  19 Time     20.7 lr = 0.162387 avg loss = 0.005407
# Epoch  19 Time     20.8 lr = 0.162387 avg loss = 0.005400
# Epoch  19 Time     20.8 lr = 0.162387 avg loss = 0.005402
# Epoch  19 Time    265.2 lr = 0.162387 avg loss = 0.005400 accuracy = 65.82
# Epoch  20 Time     37.9 lr = 0.157254 avg loss = 0.005215
# Epoch  20 Time     20.9 lr = 0.157254 avg loss = 0.005229
# Epoch  20 Time     20.8 lr = 0.157254 avg loss = 0.005254
# Epoch  20 Time     20.8 lr = 0.157254 avg loss = 0.005252
# Epoch  20 Time     20.8 lr = 0.157254 avg loss = 0.005263
# Epoch  20 Time     20.8 lr = 0.157254 avg loss = 0.005247
# Epoch  20 Time     20.9 lr = 0.157254 avg loss = 0.005264
# Epoch  20 Time     20.8 lr = 0.157254 avg loss = 0.005285
# Epoch  20 Time     20.9 lr = 0.157254 avg loss = 0.005304
# Epoch  20 Time     20.9 lr = 0.157254 avg loss = 0.005306
# Epoch  20 Time     21.1 lr = 0.157254 avg loss = 0.005315
# Epoch  20 Time     20.9 lr = 0.157254 avg loss = 0.005319
# Epoch  20 Time    267.7 lr = 0.157254 avg loss = 0.005327 accuracy = 64.02
# Epoch  21 Time     38.1 lr = 0.151887 avg loss = 0.005168
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005216
# Epoch  21 Time     21.0 lr = 0.151887 avg loss = 0.005194
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005179
# Epoch  21 Time     21.0 lr = 0.151887 avg loss = 0.005174
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005184
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005190
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005196
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005210
# Epoch  21 Time     21.0 lr = 0.151887 avg loss = 0.005231
# Epoch  21 Time     20.9 lr = 0.151887 avg loss = 0.005235
# Epoch  21 Time     21.0 lr = 0.151887 avg loss = 0.005238
# Epoch  21 Time    268.5 lr = 0.151887 avg loss = 0.005241 accuracy = 64.46
# Epoch  22 Time     38.3 lr = 0.146308 avg loss = 0.005016
# Epoch  22 Time     20.9 lr = 0.146308 avg loss = 0.005041
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005055
# Epoch  22 Time     21.1 lr = 0.146308 avg loss = 0.005086
# Epoch  22 Time     21.3 lr = 0.146308 avg loss = 0.005111
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005124
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005118
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005124
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005138
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005145
# Epoch  22 Time     21.2 lr = 0.146308 avg loss = 0.005157
# Epoch  22 Time     21.0 lr = 0.146308 avg loss = 0.005157
# Epoch  22 Time    269.8 lr = 0.146308 avg loss = 0.005152 accuracy = 66.22
# Epoch  23 Time     38.2 lr = 0.140538 avg loss = 0.005008
# Epoch  23 Time     21.0 lr = 0.140538 avg loss = 0.005031
# Epoch  23 Time     21.0 lr = 0.140538 avg loss = 0.005063
# Epoch  23 Time     20.9 lr = 0.140538 avg loss = 0.005038
# Epoch  23 Time     20.9 lr = 0.140538 avg loss = 0.005059
# Epoch  23 Time     20.9 lr = 0.140538 avg loss = 0.005062
# Epoch  23 Time     20.9 lr = 0.140538 avg loss = 0.005068
# Epoch  23 Time     20.7 lr = 0.140538 avg loss = 0.005060
# Epoch  23 Time     20.8 lr = 0.140538 avg loss = 0.005053
# Epoch  23 Time     20.8 lr = 0.140538 avg loss = 0.005052
# Epoch  23 Time     20.8 lr = 0.140538 avg loss = 0.005061
# Epoch  23 Time     20.8 lr = 0.140538 avg loss = 0.005070
# Epoch  23 Time    267.7 lr = 0.140538 avg loss = 0.005073 accuracy = 67.32
# Epoch  24 Time     38.2 lr = 0.134602 avg loss = 0.004884
# Epoch  24 Time     21.0 lr = 0.134602 avg loss = 0.004896
# Epoch  24 Time     20.9 lr = 0.134602 avg loss = 0.004890
# Epoch  24 Time     21.0 lr = 0.134602 avg loss = 0.004890
# Epoch  24 Time     20.9 lr = 0.134602 avg loss = 0.004905
# Epoch  24 Time     20.9 lr = 0.134602 avg loss = 0.004937
# Epoch  24 Time     20.8 lr = 0.134602 avg loss = 0.004956
# Epoch  24 Time     20.8 lr = 0.134602 avg loss = 0.004964
# Epoch  24 Time     20.8 lr = 0.134602 avg loss = 0.004967
# Epoch  24 Time     20.8 lr = 0.134602 avg loss = 0.004970
# Epoch  24 Time     20.7 lr = 0.134602 avg loss = 0.004972
# Epoch  24 Time     20.7 lr = 0.134602 avg loss = 0.004976
# Epoch  24 Time    267.2 lr = 0.134602 avg loss = 0.004975 accuracy = 67.16
# Epoch  25 Time     37.7 lr = 0.128524 avg loss = 0.004945
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004867
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004840
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004861
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004882
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004890
# Epoch  25 Time     20.8 lr = 0.128524 avg loss = 0.004900
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004907
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004920
# Epoch  25 Time     20.8 lr = 0.128524 avg loss = 0.004920
# Epoch  25 Time     20.8 lr = 0.128524 avg loss = 0.004917
# Epoch  25 Time     20.7 lr = 0.128524 avg loss = 0.004914
# Epoch  25 Time    265.6 lr = 0.128524 avg loss = 0.004915 accuracy = 65.88
# Epoch  26 Time     37.6 lr = 0.122330 avg loss = 0.004722
# Epoch  26 Time     20.9 lr = 0.122330 avg loss = 0.004754
# Epoch  26 Time     21.0 lr = 0.122330 avg loss = 0.004776
# Epoch  26 Time     20.9 lr = 0.122330 avg loss = 0.004747
# Epoch  26 Time     20.7 lr = 0.122330 avg loss = 0.004749
# Epoch  26 Time     20.7 lr = 0.122330 avg loss = 0.004773
# Epoch  26 Time     20.9 lr = 0.122330 avg loss = 0.004782
# Epoch  26 Time     20.8 lr = 0.122330 avg loss = 0.004771
# Epoch  26 Time     20.9 lr = 0.122330 avg loss = 0.004772
# Epoch  26 Time     20.8 lr = 0.122330 avg loss = 0.004785
# Epoch  26 Time     20.8 lr = 0.122330 avg loss = 0.004799
# Epoch  26 Time     20.9 lr = 0.122330 avg loss = 0.004804
# Epoch  26 Time    267.3 lr = 0.122330 avg loss = 0.004813 accuracy = 65.58
# Epoch  27 Time     37.8 lr = 0.116044 avg loss = 0.004609
# Epoch  27 Time     20.9 lr = 0.116044 avg loss = 0.004658
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004681
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004674
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004685
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004694
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004698
# Epoch  27 Time     21.0 lr = 0.116044 avg loss = 0.004706
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004723
# Epoch  27 Time     20.9 lr = 0.116044 avg loss = 0.004725
# Epoch  27 Time     20.9 lr = 0.116044 avg loss = 0.004742
# Epoch  27 Time     20.8 lr = 0.116044 avg loss = 0.004751
# Epoch  27 Time    267.1 lr = 0.116044 avg loss = 0.004750 accuracy = 65.32
# Epoch  28 Time     38.1 lr = 0.109693 avg loss = 0.004483
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004520
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004550
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004582
# Epoch  28 Time     20.8 lr = 0.109693 avg loss = 0.004589
# Epoch  28 Time     20.8 lr = 0.109693 avg loss = 0.004603
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004606
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004625
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004633
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004632
# Epoch  28 Time     20.9 lr = 0.109693 avg loss = 0.004643
# Epoch  28 Time     21.0 lr = 0.109693 avg loss = 0.004658
# Epoch  28 Time    268.3 lr = 0.109693 avg loss = 0.004660 accuracy = 67.34
# Epoch  29 Time     38.1 lr = 0.103302 avg loss = 0.004540
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004513
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004507
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004504
# Epoch  29 Time     21.0 lr = 0.103302 avg loss = 0.004518
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004522
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004531
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004537
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004553
# Epoch  29 Time     20.8 lr = 0.103302 avg loss = 0.004554
# Epoch  29 Time     20.9 lr = 0.103302 avg loss = 0.004559
# Epoch  29 Time     20.7 lr = 0.103302 avg loss = 0.004569
# Epoch  29 Time    267.6 lr = 0.103302 avg loss = 0.004572 accuracy = 67.54
# Epoch  30 Time     37.8 lr = 0.096898 avg loss = 0.004298
# Epoch  30 Time     20.8 lr = 0.096898 avg loss = 0.004346
# Epoch  30 Time     20.7 lr = 0.096898 avg loss = 0.004385
# Epoch  30 Time     20.6 lr = 0.096898 avg loss = 0.004377
# Epoch  30 Time     20.7 lr = 0.096898 avg loss = 0.004406
# Epoch  30 Time     20.7 lr = 0.096898 avg loss = 0.004438
# Epoch  30 Time     20.5 lr = 0.096898 avg loss = 0.004462
# Epoch  30 Time     20.6 lr = 0.096898 avg loss = 0.004479
# Epoch  30 Time     20.6 lr = 0.096898 avg loss = 0.004489
# Epoch  30 Time     20.7 lr = 0.096898 avg loss = 0.004495
# Epoch  30 Time     20.7 lr = 0.096898 avg loss = 0.004503
# Epoch  30 Time     20.6 lr = 0.096898 avg loss = 0.004508
# Epoch  30 Time    264.9 lr = 0.096898 avg loss = 0.004510 accuracy = 67.82
# Epoch  31 Time     37.6 lr = 0.090507 avg loss = 0.004306
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004361
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004355
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004348
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004366
# Epoch  31 Time     20.6 lr = 0.090507 avg loss = 0.004370
# Epoch  31 Time     20.6 lr = 0.090507 avg loss = 0.004372
# Epoch  31 Time     20.6 lr = 0.090507 avg loss = 0.004372
# Epoch  31 Time     20.8 lr = 0.090507 avg loss = 0.004378
# Epoch  31 Time     20.8 lr = 0.090507 avg loss = 0.004385
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004393
# Epoch  31 Time     20.7 lr = 0.090507 avg loss = 0.004406
# Epoch  31 Time    265.6 lr = 0.090507 avg loss = 0.004414 accuracy = 68.86
# Epoch  32 Time     38.1 lr = 0.084156 avg loss = 0.004174
# Epoch  32 Time     20.7 lr = 0.084156 avg loss = 0.004193
# Epoch  32 Time     20.6 lr = 0.084156 avg loss = 0.004228
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004275
# Epoch  32 Time     20.6 lr = 0.084156 avg loss = 0.004306
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004312
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004330
# Epoch  32 Time     20.6 lr = 0.084156 avg loss = 0.004323
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004327
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004331
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004327
# Epoch  32 Time     20.5 lr = 0.084156 avg loss = 0.004334
# Epoch  32 Time    263.6 lr = 0.084156 avg loss = 0.004339 accuracy = 67.30
# Epoch  33 Time     37.3 lr = 0.077870 avg loss = 0.004207
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004156
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004144
# Epoch  33 Time     20.5 lr = 0.077870 avg loss = 0.004166
# Epoch  33 Time     20.5 lr = 0.077870 avg loss = 0.004160
# Epoch  33 Time     20.5 lr = 0.077870 avg loss = 0.004182
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004204
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004210
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004213
# Epoch  33 Time     20.5 lr = 0.077870 avg loss = 0.004218
# Epoch  33 Time     20.5 lr = 0.077870 avg loss = 0.004217
# Epoch  33 Time     20.6 lr = 0.077870 avg loss = 0.004223
# Epoch  33 Time    263.7 lr = 0.077870 avg loss = 0.004233 accuracy = 69.72
# Epoch  34 Time     37.4 lr = 0.071676 avg loss = 0.004113
# Epoch  34 Time     20.6 lr = 0.071676 avg loss = 0.004118
# Epoch  34 Time     20.6 lr = 0.071676 avg loss = 0.004131
# Epoch  34 Time     20.5 lr = 0.071676 avg loss = 0.004133
# Epoch  34 Time     20.6 lr = 0.071676 avg loss = 0.004136
# Epoch  34 Time     20.7 lr = 0.071676 avg loss = 0.004130
# Epoch  34 Time     20.4 lr = 0.071676 avg loss = 0.004139
# Epoch  34 Time     20.5 lr = 0.071676 avg loss = 0.004151
# Epoch  34 Time     20.4 lr = 0.071676 avg loss = 0.004151
# Epoch  34 Time     20.5 lr = 0.071676 avg loss = 0.004150
# Epoch  34 Time     20.7 lr = 0.071676 avg loss = 0.004151
# Epoch  34 Time     20.5 lr = 0.071676 avg loss = 0.004157
# Epoch  34 Time    263.5 lr = 0.071676 avg loss = 0.004163 accuracy = 69.14
# Epoch  35 Time     37.7 lr = 0.065598 avg loss = 0.003919
# Epoch  35 Time     20.9 lr = 0.065598 avg loss = 0.003935
# Epoch  35 Time     21.0 lr = 0.065598 avg loss = 0.003976
# Epoch  35 Time     20.8 lr = 0.065598 avg loss = 0.003993
# Epoch  35 Time     20.8 lr = 0.065598 avg loss = 0.004009
# Epoch  35 Time     20.8 lr = 0.065598 avg loss = 0.004003
# Epoch  35 Time     20.6 lr = 0.065598 avg loss = 0.004029
# Epoch  35 Time     20.7 lr = 0.065598 avg loss = 0.004036
# Epoch  35 Time     20.9 lr = 0.065598 avg loss = 0.004043
# Epoch  35 Time     20.9 lr = 0.065598 avg loss = 0.004049
# Epoch  35 Time     21.0 lr = 0.065598 avg loss = 0.004057
# Epoch  35 Time     20.9 lr = 0.065598 avg loss = 0.004060
# Epoch  35 Time    267.3 lr = 0.065598 avg loss = 0.004065 accuracy = 70.40
# Epoch  36 Time     38.2 lr = 0.059662 avg loss = 0.003971
# Epoch  36 Time     20.8 lr = 0.059662 avg loss = 0.003945
# Epoch  36 Time     20.9 lr = 0.059662 avg loss = 0.003955
# Epoch  36 Time     20.6 lr = 0.059662 avg loss = 0.003955
# Epoch  36 Time     20.7 lr = 0.059662 avg loss = 0.003945
# Epoch  36 Time     20.7 lr = 0.059662 avg loss = 0.003967
# Epoch  36 Time     20.7 lr = 0.059662 avg loss = 0.003974
# Epoch  36 Time     20.7 lr = 0.059662 avg loss = 0.003982
# Epoch  36 Time     20.6 lr = 0.059662 avg loss = 0.003981
# Epoch  36 Time     20.7 lr = 0.059662 avg loss = 0.003993
# Epoch  36 Time     20.8 lr = 0.059662 avg loss = 0.003990
# Epoch  36 Time     20.8 lr = 0.059662 avg loss = 0.003999
# Epoch  36 Time    265.9 lr = 0.059662 avg loss = 0.004004 accuracy = 71.84
# Epoch  37 Time     37.9 lr = 0.053892 avg loss = 0.003903
# Epoch  37 Time     20.9 lr = 0.053892 avg loss = 0.003907
# Epoch  37 Time     20.8 lr = 0.053892 avg loss = 0.003873
# Epoch  37 Time     20.9 lr = 0.053892 avg loss = 0.003874
# Epoch  37 Time     20.9 lr = 0.053892 avg loss = 0.003882
# Epoch  37 Time     21.1 lr = 0.053892 avg loss = 0.003887
# Epoch  37 Time     21.1 lr = 0.053892 avg loss = 0.003883
# Epoch  37 Time     20.8 lr = 0.053892 avg loss = 0.003880
# Epoch  37 Time     20.8 lr = 0.053892 avg loss = 0.003881
# Epoch  37 Time     20.9 lr = 0.053892 avg loss = 0.003883
# Epoch  37 Time     20.9 lr = 0.053892 avg loss = 0.003886
# Epoch  37 Time     20.8 lr = 0.053892 avg loss = 0.003888
# Epoch  37 Time    268.0 lr = 0.053892 avg loss = 0.003893 accuracy = 70.72
# Epoch  38 Time     37.8 lr = 0.048313 avg loss = 0.003654
# Epoch  38 Time     20.8 lr = 0.048313 avg loss = 0.003687
# Epoch  38 Time     20.8 lr = 0.048313 avg loss = 0.003713
# Epoch  38 Time     20.9 lr = 0.048313 avg loss = 0.003743
# Epoch  38 Time     20.7 lr = 0.048313 avg loss = 0.003743
# Epoch  38 Time     20.7 lr = 0.048313 avg loss = 0.003767
# Epoch  38 Time     20.8 lr = 0.048313 avg loss = 0.003775
# Epoch  38 Time     20.8 lr = 0.048313 avg loss = 0.003784
# Epoch  38 Time     20.7 lr = 0.048313 avg loss = 0.003790
# Epoch  38 Time     20.7 lr = 0.048313 avg loss = 0.003791
# Epoch  38 Time     20.8 lr = 0.048313 avg loss = 0.003801
# Epoch  38 Time     20.7 lr = 0.048313 avg loss = 0.003798
# Epoch  38 Time    265.9 lr = 0.048313 avg loss = 0.003802 accuracy = 71.04
# Epoch  39 Time     37.8 lr = 0.042946 avg loss = 0.003794
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003708
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003692
# Epoch  39 Time     20.8 lr = 0.042946 avg loss = 0.003686
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003672
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003669
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003685
# Epoch  39 Time     20.8 lr = 0.042946 avg loss = 0.003688
# Epoch  39 Time     20.6 lr = 0.042946 avg loss = 0.003704
# Epoch  39 Time     20.6 lr = 0.042946 avg loss = 0.003705
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003717
# Epoch  39 Time     20.7 lr = 0.042946 avg loss = 0.003720
# Epoch  39 Time    265.4 lr = 0.042946 avg loss = 0.003723 accuracy = 71.72
# Epoch  40 Time     37.7 lr = 0.037813 avg loss = 0.003624
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003625
# Epoch  40 Time     20.8 lr = 0.037813 avg loss = 0.003618
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003607
# Epoch  40 Time     20.6 lr = 0.037813 avg loss = 0.003629
# Epoch  40 Time     20.8 lr = 0.037813 avg loss = 0.003622
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003636
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003620
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003632
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003630
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003630
# Epoch  40 Time     20.7 lr = 0.037813 avg loss = 0.003635
# Epoch  40 Time    265.5 lr = 0.037813 avg loss = 0.003641 accuracy = 70.54
# Epoch  41 Time     37.6 lr = 0.032937 avg loss = 0.003482
# Epoch  41 Time     20.8 lr = 0.032937 avg loss = 0.003481
# Epoch  41 Time     21.1 lr = 0.032937 avg loss = 0.003467
# Epoch  41 Time     21.0 lr = 0.032937 avg loss = 0.003479
# Epoch  41 Time     20.9 lr = 0.032937 avg loss = 0.003508
# Epoch  41 Time     20.7 lr = 0.032937 avg loss = 0.003520
# Epoch  41 Time     20.7 lr = 0.032937 avg loss = 0.003529
# Epoch  41 Time     20.8 lr = 0.032937 avg loss = 0.003525
# Epoch  41 Time     20.8 lr = 0.032937 avg loss = 0.003535
# Epoch  41 Time     20.8 lr = 0.032937 avg loss = 0.003544
# Epoch  41 Time     20.9 lr = 0.032937 avg loss = 0.003543
# Epoch  41 Time     20.8 lr = 0.032937 avg loss = 0.003538
# Epoch  41 Time    267.0 lr = 0.032937 avg loss = 0.003542 accuracy = 72.12
# Epoch  42 Time     38.0 lr = 0.028337 avg loss = 0.003390
# Epoch  42 Time     21.1 lr = 0.028337 avg loss = 0.003419
# Epoch  42 Time     20.9 lr = 0.028337 avg loss = 0.003406
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003412
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003420
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003420
# Epoch  42 Time     20.9 lr = 0.028337 avg loss = 0.003422
# Epoch  42 Time     20.9 lr = 0.028337 avg loss = 0.003421
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003425
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003432
# Epoch  42 Time     20.8 lr = 0.028337 avg loss = 0.003435
# Epoch  42 Time     20.9 lr = 0.028337 avg loss = 0.003437
# Epoch  42 Time    267.6 lr = 0.028337 avg loss = 0.003443 accuracy = 72.54
# Epoch  43 Time     38.0 lr = 0.024032 avg loss = 0.003285
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003311
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003311
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003345
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003350
# Epoch  43 Time     20.9 lr = 0.024032 avg loss = 0.003365
# Epoch  43 Time     20.9 lr = 0.024032 avg loss = 0.003370
# Epoch  43 Time     20.9 lr = 0.024032 avg loss = 0.003356
# Epoch  43 Time     20.9 lr = 0.024032 avg loss = 0.003357
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003355
# Epoch  43 Time     20.8 lr = 0.024032 avg loss = 0.003366
# Epoch  43 Time     20.9 lr = 0.024032 avg loss = 0.003373
# Epoch  43 Time    267.5 lr = 0.024032 avg loss = 0.003381 accuracy = 73.00
# Epoch  44 Time     38.3 lr = 0.020039 avg loss = 0.003309
# Epoch  44 Time     20.9 lr = 0.020039 avg loss = 0.003296
# Epoch  44 Time     20.9 lr = 0.020039 avg loss = 0.003260
# Epoch  44 Time     20.7 lr = 0.020039 avg loss = 0.003253
# Epoch  44 Time     20.6 lr = 0.020039 avg loss = 0.003264
# Epoch  44 Time     20.6 lr = 0.020039 avg loss = 0.003271
# Epoch  44 Time     20.5 lr = 0.020039 avg loss = 0.003277
# Epoch  44 Time     20.4 lr = 0.020039 avg loss = 0.003278
# Epoch  44 Time     20.5 lr = 0.020039 avg loss = 0.003280
# Epoch  44 Time     20.4 lr = 0.020039 avg loss = 0.003287
# Epoch  44 Time     20.4 lr = 0.020039 avg loss = 0.003292
# Epoch  44 Time     20.5 lr = 0.020039 avg loss = 0.003302
# Epoch  44 Time    264.1 lr = 0.020039 avg loss = 0.003301 accuracy = 72.84
# Epoch  45 Time     37.1 lr = 0.016375 avg loss = 0.003320
# Epoch  45 Time     20.4 lr = 0.016375 avg loss = 0.003239
# Epoch  45 Time     20.3 lr = 0.016375 avg loss = 0.003249
# Epoch  45 Time     20.5 lr = 0.016375 avg loss = 0.003211
# Epoch  45 Time     20.3 lr = 0.016375 avg loss = 0.003204
# Epoch  45 Time     20.3 lr = 0.016375 avg loss = 0.003209
# Epoch  45 Time     20.4 lr = 0.016375 avg loss = 0.003215
# Epoch  45 Time     20.4 lr = 0.016375 avg loss = 0.003216
# Epoch  45 Time     20.4 lr = 0.016375 avg loss = 0.003227
# Epoch  45 Time     20.3 lr = 0.016375 avg loss = 0.003231
# Epoch  45 Time     20.2 lr = 0.016375 avg loss = 0.003235
# Epoch  45 Time     20.4 lr = 0.016375 avg loss = 0.003241
# Epoch  45 Time    261.2 lr = 0.016375 avg loss = 0.003239 accuracy = 72.56
# Epoch  46 Time     37.1 lr = 0.013055 avg loss = 0.003174
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003140
# Epoch  46 Time     20.3 lr = 0.013055 avg loss = 0.003142
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003145
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003148
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003151
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003153
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003155
# Epoch  46 Time     20.4 lr = 0.013055 avg loss = 0.003161
# Epoch  46 Time     20.5 lr = 0.013055 avg loss = 0.003164
# Epoch  46 Time     20.3 lr = 0.013055 avg loss = 0.003168
# Epoch  46 Time     20.3 lr = 0.013055 avg loss = 0.003168
# Epoch  46 Time    261.1 lr = 0.013055 avg loss = 0.003161 accuracy = 73.40
# Epoch  47 Time     37.0 lr = 0.010093 avg loss = 0.003120
# Epoch  47 Time     20.3 lr = 0.010093 avg loss = 0.003106
# Epoch  47 Time     20.4 lr = 0.010093 avg loss = 0.003100
# Epoch  47 Time     20.5 lr = 0.010093 avg loss = 0.003109
# Epoch  47 Time     20.3 lr = 0.010093 avg loss = 0.003109
# Epoch  47 Time     20.3 lr = 0.010093 avg loss = 0.003124
# Epoch  47 Time     20.4 lr = 0.010093 avg loss = 0.003119
# Epoch  47 Time     20.3 lr = 0.010093 avg loss = 0.003118
# Epoch  47 Time     20.4 lr = 0.010093 avg loss = 0.003121
# Epoch  47 Time     20.4 lr = 0.010093 avg loss = 0.003114
# Epoch  47 Time     20.4 lr = 0.010093 avg loss = 0.003112
# Epoch  47 Time     20.5 lr = 0.010093 avg loss = 0.003116
# Epoch  47 Time    261.2 lr = 0.010093 avg loss = 0.003119 accuracy = 73.52
# Epoch  48 Time     37.2 lr = 0.007501 avg loss = 0.003040
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003076
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003040
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003050
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003044
# Epoch  48 Time     20.5 lr = 0.007501 avg loss = 0.003041
# Epoch  48 Time     20.5 lr = 0.007501 avg loss = 0.003048
# Epoch  48 Time     20.5 lr = 0.007501 avg loss = 0.003059
# Epoch  48 Time     20.5 lr = 0.007501 avg loss = 0.003067
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003065
# Epoch  48 Time     20.3 lr = 0.007501 avg loss = 0.003069
# Epoch  48 Time     20.6 lr = 0.007501 avg loss = 0.003073
# Epoch  48 Time    261.5 lr = 0.007501 avg loss = 0.003073 accuracy = 73.78
# Epoch  49 Time     37.2 lr = 0.005289 avg loss = 0.002979
# Epoch  49 Time     20.5 lr = 0.005289 avg loss = 0.003005
# Epoch  49 Time     20.4 lr = 0.005289 avg loss = 0.002999
# Epoch  49 Time     20.3 lr = 0.005289 avg loss = 0.003008
# Epoch  49 Time     20.4 lr = 0.005289 avg loss = 0.002989
# Epoch  49 Time     20.5 lr = 0.005289 avg loss = 0.002995
# Epoch  49 Time     20.4 lr = 0.005289 avg loss = 0.002993
# Epoch  49 Time     20.5 lr = 0.005289 avg loss = 0.003003
# Epoch  49 Time     20.4 lr = 0.005289 avg loss = 0.003013
# Epoch  49 Time     20.4 lr = 0.005289 avg loss = 0.003017
# Epoch  49 Time     20.6 lr = 0.005289 avg loss = 0.003026
# Epoch  49 Time     20.5 lr = 0.005289 avg loss = 0.003024
# Epoch  49 Time    262.4 lr = 0.005289 avg loss = 0.003021 accuracy = 73.00
# Epoch  50 Time     37.5 lr = 0.003467 avg loss = 0.002982
# Epoch  50 Time     20.6 lr = 0.003467 avg loss = 0.002993
# Epoch  50 Time     20.5 lr = 0.003467 avg loss = 0.003006
# Epoch  50 Time     20.5 lr = 0.003467 avg loss = 0.003007
# Epoch  50 Time     20.4 lr = 0.003467 avg loss = 0.002995
# Epoch  50 Time     20.4 lr = 0.003467 avg loss = 0.002994
# Epoch  50 Time     20.5 lr = 0.003467 avg loss = 0.002983
# Epoch  50 Time     20.5 lr = 0.003467 avg loss = 0.002995
# Epoch  50 Time     20.6 lr = 0.003467 avg loss = 0.002992
# Epoch  50 Time     20.5 lr = 0.003467 avg loss = 0.003001
# Epoch  50 Time     20.6 lr = 0.003467 avg loss = 0.002996
# Epoch  50 Time     20.4 lr = 0.003467 avg loss = 0.002989
# Epoch  50 Time    262.8 lr = 0.003467 avg loss = 0.002986 accuracy = 73.68
# Epoch  51 Time     37.1 lr = 0.002042 avg loss = 0.003013
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002997
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002997
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002983
# Epoch  51 Time     20.4 lr = 0.002042 avg loss = 0.002966
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002959
# Epoch  51 Time     20.4 lr = 0.002042 avg loss = 0.002987
# Epoch  51 Time     20.4 lr = 0.002042 avg loss = 0.002986
# Epoch  51 Time     20.6 lr = 0.002042 avg loss = 0.002987
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002992
# Epoch  51 Time     20.6 lr = 0.002042 avg loss = 0.002988
# Epoch  51 Time     20.5 lr = 0.002042 avg loss = 0.002993
# Epoch  51 Time    262.6 lr = 0.002042 avg loss = 0.002995 accuracy = 73.64
# Epoch  52 Time     37.3 lr = 0.001020 avg loss = 0.003024
# Epoch  52 Time     20.6 lr = 0.001020 avg loss = 0.002995
# Epoch  52 Time     20.6 lr = 0.001020 avg loss = 0.002998
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002985
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002993
# Epoch  52 Time     20.7 lr = 0.001020 avg loss = 0.002981
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002972
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002960
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002954
# Epoch  52 Time     20.6 lr = 0.001020 avg loss = 0.002961
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002961
# Epoch  52 Time     20.5 lr = 0.001020 avg loss = 0.002956
# Epoch  52 Time    263.3 lr = 0.001020 avg loss = 0.002960 accuracy = 73.72
# Epoch  53 Time     37.2 lr = 0.000405 avg loss = 0.002909
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002960
# Epoch  53 Time     20.6 lr = 0.000405 avg loss = 0.002977
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002961
# Epoch  53 Time     20.4 lr = 0.000405 avg loss = 0.002972
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002966
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002955
# Epoch  53 Time     20.6 lr = 0.000405 avg loss = 0.002943
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002936
# Epoch  53 Time     20.6 lr = 0.000405 avg loss = 0.002938
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002937
# Epoch  53 Time     20.5 lr = 0.000405 avg loss = 0.002935
# Epoch  53 Time    263.0 lr = 0.000405 avg loss = 0.002934 accuracy = 73.76
# Epoch  54 Time     37.2 lr = 0.000200 avg loss = 0.002918
# Epoch  54 Time     20.5 lr = 0.000200 avg loss = 0.002941
# Epoch  54 Time     20.6 lr = 0.000200 avg loss = 0.002934
# Epoch  54 Time     20.4 lr = 0.000200 avg loss = 0.002919
# Epoch  54 Time     20.6 lr = 0.000200 avg loss = 0.002918
# Epoch  54 Time     20.5 lr = 0.000200 avg loss = 0.002918
# Epoch  54 Time     20.5 lr = 0.000200 avg loss = 0.002920
# Epoch  54 Time     20.5 lr = 0.000200 avg loss = 0.002922
# Epoch  54 Time     20.5 lr = 0.000200 avg loss = 0.002929
# Epoch  54 Time     20.6 lr = 0.000200 avg loss = 0.002935
# Epoch  54 Time     20.7 lr = 0.000200 avg loss = 0.002935
# Epoch  54 Time     20.7 lr = 0.000200 avg loss = 0.002939
# Epoch  54 Time    263.6 lr = 0.000200 avg loss = 0.002933 accuracy = 74.04
#
#    3. Submit eff.py, eff_se.py (if done) and eff_se_cond.py (if done) via
#       eLearning (no zip files, no Jupyter / iPython notebooks, ...) with this
#       comment block at the top and all code from the IMPORT comment block to
#       the end; so if you implement all 3, you will submit 3 Python files
#
# HELP
#
#    1. If you're looking for a reference for block and network design, see
#       https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/blob/master/Tests/202008/xNNs_Project_002_Networks.py
#       which implemented a RegNetX style block and network; while the block and
#       network is different, that code should help with thinking about how to
#       organize this code
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

# torch
import torch
import torch.nn       as     nn
import torch.optim    as     optim
from   torch.autograd import Function

# torch utils
import torchvision
import torchvision.transforms as transforms

# additional libraries
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt

################################################################################
#
# PARAMETERS
#
################################################################################

# data
DATA_DIR_1        = 'data'
DATA_DIR_2        = 'data/imagenet64'
DATA_DIR_TRAIN    = 'data/imagenet64/train'
DATA_DIR_TEST     = 'data/imagenet64/val'
DATA_FILE_TRAIN_1 = 'Train1.zip'
DATA_FILE_TRAIN_2 = 'Train2.zip'
DATA_FILE_TRAIN_3 = 'Train3.zip'
DATA_FILE_TRAIN_4 = 'Train4.zip'
DATA_FILE_TRAIN_5 = 'Train5.zip'
DATA_FILE_TEST_1  = 'Val1.zip'
DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'
DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'
DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'
DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'
DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'
DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'
DATA_BATCH_SIZE   = 256
DATA_NUM_WORKERS  = 4
DATA_NUM_CHANNELS = 3
DATA_NUM_CLASSES  = 100
DATA_RESIZE       = 64
DATA_CROP         = 56
DATA_MEAN         = (0.485, 0.456, 0.406)
DATA_STD_DEV      = (0.229, 0.224, 0.225)
rank_reduction_ratio = 4
# model
MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation
MODEL_LEVEL_2_BLOCKS   = 1
MODEL_LEVEL_3_BLOCKS   = 2
MODEL_LEVEL_4_BLOCKS   = 3
MODEL_LEVEL_5_BLOCKS   = 4
MODEL_LEVEL_1_CHANNELS = 16
MODEL_LEVEL_2_CHANNELS = 24
MODEL_LEVEL_3_CHANNELS = 40
MODEL_LEVEL_4_CHANNELS = 80
MODEL_LEVEL_5_CHANNELS = 160
MODEL_LEVEL_6_CHANNELS = 320
MODEL_LEVEL_7_CHANNELS = 1280

# training
TRAIN_LR_MAX              = 0.2
TRAIN_LR_INIT_SCALE       = 0.01
TRAIN_LR_FINAL_SCALE      = 0.001
TRAIN_LR_INIT_EPOCHS      = 5
TRAIN_LR_FINAL_EPOCHS     = 50 # 100
TRAIN_NUM_EPOCHS          = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS
TRAIN_LR_INIT             = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE
TRAIN_LR_FINAL            = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE
TRAIN_INTRA_EPOCH_DISPLAY = 10000

# file
FILE_NAME_CHECK      = 'EffNetStyleCheck.pt'
FILE_NAME_BEST       = 'EffNetStyleBest.pt'
FILE_SAVE            = True
FILE_LOAD            = False
FILE_EXTEND_TRAINING = False
FILE_NEW_OPTIMIZER   = False

################################################################################
#
# DATA
#
################################################################################

# create a local directory structure for data storage
if (os.path.exists(DATA_DIR_1) == False):
    os.mkdir(DATA_DIR_1)
if (os.path.exists(DATA_DIR_2) == False):
    os.mkdir(DATA_DIR_2)
if (os.path.exists(DATA_DIR_TRAIN) == False):
    os.mkdir(DATA_DIR_TRAIN)
if (os.path.exists(DATA_DIR_TEST) == False):
    os.mkdir(DATA_DIR_TEST)

# download data
if (os.path.exists(DATA_FILE_TRAIN_1) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)
if (os.path.exists(DATA_FILE_TRAIN_2) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)
if (os.path.exists(DATA_FILE_TRAIN_3) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)
if (os.path.exists(DATA_FILE_TRAIN_4) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)
if (os.path.exists(DATA_FILE_TRAIN_5) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)
if (os.path.exists(DATA_FILE_TEST_1) == False):
    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)

# extract data
with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TEST)

# transforms
transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])
transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])

# data sets
dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)
dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)

# data loader
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True)
dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False)

################################################################################
#
# NETWORK BUILDING BLOCK
#
################################################################################
#SE
class SqueezeExcitation(nn.Module):
  def __init__(self,Ne,R):
    super(SqueezeExcitation,self).__init__()
    rankReducer = int(Ne/R)
    self.avgPool = nn.AdaptiveAvgPool2d((1, 1))
    self.flat = nn.Flatten()
    self.linear = nn.Linear(Ne,rankReducer,bias=True)
    self.reluSE = nn.ReLU()
    self.linear2 =nn.Linear(rankReducer,Ne,bias=True)
    self.sig = nn.Sigmoid()

  def forward(self,x):
    batch = x.shape[0]
    ne = x.shape[1]
    se = self.avgPool(x)
    se = self.flat(se)
    se = self.linear(se)
    se = self.reluSE(se)
    se = self.linear2(se)
    se = self.sig(se)
    se = se.reshape([batch,ne,1,1])
    se = x *se
    return se
#################################################################################################
# inverted residual block
class InvResBlock(nn.Module):

    # initialization
    def __init__(self, Ni, Ne, No, F, S,R):

        # parent initialization
        super(InvResBlock, self).__init__()

        # create all of the operators for the inverted residual block in fig 2a
        # of the paper; note that parameter names were chosen to match the paper

    #identity:
        if((Ni == No) and S==1):
          self.id =True
       # self.conv0 = nn.Conv2d(Ni,No,(3,3),stride=S,bias=False)
        #self.bn0 = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        else:
        #residual
          self.id = False

        P = np.floor(F/2).astype(int)
        self.conv1 = nn.Conv2d(Ni, Ne, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros')
        self.bn1   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(Ne, Ne, kernel_size=F, stride=S, padding=P, dilation=(1, 1), groups=Ne, bias=False, padding_mode='zeros')
        self.bn2   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.relu2 = nn.ReLU()
        #SE enhanced
        self.SqueezeExcitation = SqueezeExcitation(Ne,R)
        ##SE ^^^^
        self.conv3 = nn.Conv2d(Ne, No, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, bias=False, padding_mode='zeros')
        self.bn3   = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

        #sum
        #self.relu0 = nn.ReLU()
    # forward path
    def forward(self, x):
        # identity
        if (self.id == True):
            id = x

        # residual
        res = self.conv1(x)
        res = self.bn1(res)
        res = self.relu1(res)
        res = self.conv2(res)
  #      print(res.shape)
        res = self.bn2(res)
        res = self.relu2(res)
   #     print('res shape enter',res.shape)
        #SE
        res = self.SqueezeExcitation(res)
        #SE ^^^
        res = self.conv3(res)
        res = self.bn3(res)
        # sum
        if(self.id==True):
          y = id + res
        else:
          y=res
        #y = self.relu0(y)

        # return
        return y
################################################################################
#
# NETWORK
#
################################################################################

# define
class Model(nn.Module):

    # initialization
    # add necessary parameters to the init function to create the model defined
    # in table 1 of the paper
    # initialization
    def __init__(self,
                 data_num_channels,
                 model_level_1_blocks, model_level_1_channels,
                 model_level_2_blocks, model_level_2_channels,
                 model_level_3_blocks, model_level_3_channels, 
                 model_level_4_blocks, model_level_4_channels, 
                 model_level_5_blocks, model_level_5_channels,
                 model_level_6_channels,model_level_7_channels, 
                 data_num_classes):

        # parent initialization
        super(Model, self).__init__()

        # create all of the operators for the network defined in table 1 of the
        # paper using a combination of Python, standard PyTorch operators and
        # the previously defined InvResBlock class
        # <TO DO: your code goes here>

        # stride
        stride1 = 1 
        stride2 = 2 
        stride3 = 2
        stride4 = 2
        stride5 = 1

        # encoder level 1 - Tail
        self.tail = nn.ModuleList()
        self.tail.append(nn.Conv2d(data_num_channels, model_level_1_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros'))
        self.tail.append(nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.tail.append(nn.ReLU())

 

        # encoder level 2 Body 1
        self.enc_1 = nn.ModuleList()
        for n in range(model_level_1_blocks-1):
            self.enc_1.append(InvResBlock(model_level_1_channels, model_level_1_channels * 4, model_level_1_channels, 3, 1,rank_reduction_ratio))
        self.enc_1.append(InvResBlock(model_level_1_channels, model_level_1_channels * 4, model_level_2_channels, 3, stride1,rank_reduction_ratio))
        #print("phase 1").


        # encoder level 3 Body 2
        self.enc_2 = nn.ModuleList()
        for n in range(model_level_2_blocks-1):
            self.enc_2.append(InvResBlock(model_level_2_channels, model_level_2_channels * 4, model_level_2_channels, 3, 1,rank_reduction_ratio))
        self.enc_2.append(InvResBlock(model_level_2_channels, model_level_2_channels * 4, model_level_3_channels, 3, stride2,rank_reduction_ratio))
        #print("phase 2")

        # encoder level 4 Body 3
        self.enc_3 = nn.ModuleList()
        for n in range(model_level_3_blocks-1):
            self.enc_3.append(InvResBlock(model_level_3_channels, model_level_3_channels * 4, model_level_3_channels, 3, 1,rank_reduction_ratio))        
        self.enc_3.append(InvResBlock(model_level_3_channels, model_level_3_channels * 4, model_level_4_channels, 3, stride3,rank_reduction_ratio))

        
         # encoder level 5  Body 4
        self.enc_4 = nn.ModuleList()
        for n in range(model_level_4_blocks-1):
            self.enc_4.append(InvResBlock(model_level_4_channels, model_level_4_channels * 4, model_level_4_channels, 3, 1, rank_reduction_ratio))
        self.enc_4.append(InvResBlock(model_level_4_channels, model_level_4_channels * 4, model_level_5_channels, 3, stride4, rank_reduction_ratio))


        # encoder level 6  Body 5
        self.enc_5 = nn.ModuleList()
        for n in range(model_level_5_blocks-1):
            self.enc_5.append(InvResBlock(model_level_5_channels, model_level_5_channels * 4, model_level_5_channels, 3, 1, rank_reduction_ratio))
        self.enc_5.append(InvResBlock(model_level_5_channels, model_level_5_channels * 4, model_level_6_channels, 3, stride5, rank_reduction_ratio))

        


        # decoder
        self.dec = nn.ModuleList()
        self.dec.append(nn.Conv2d(model_level_6_channels, model_level_7_channels, 1, stride=1, groups=1, bias=False))
        self.dec.append(nn.BatchNorm2d(model_level_7_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))
        self.dec.append(nn.ReLU())
        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))
        self.dec.append(nn.Flatten())
        self.dec.append(nn.Linear(model_level_7_channels, data_num_classes, bias=True))

    # forward path
    def forward(self, x):
        #Tail part:
        for layer in self.tail:
          x=layer(x)
        # encoder level 1
        for layer in self.enc_1:
            x = layer(x)

        # encoder level 2
        for layer in self.enc_2:
            x = layer(x)

        # encoder level 3
        for layer in self.enc_3:
            x = layer(x)

        # encoder level 4
        for layer in self.enc_4:
            x = layer(x)

        # encoder level 5
        for layer in self.enc_5:
            x = layer(x)

        # decoder
        for layer in self.dec:
            x = layer(x)

        # map input x to output y for the network defined in table 1 of the
        # paper via connecting the operators defined in the initialization
        # and return output y
      
        y=x
        # return
        return y

# create
# add necessary parameters to the init function to create the model defined
# in table 1 of the paper
model = Model(DATA_NUM_CHANNELS,
              MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_1_CHANNELS, 
              MODEL_LEVEL_2_BLOCKS, MODEL_LEVEL_2_CHANNELS, 
              MODEL_LEVEL_3_BLOCKS, MODEL_LEVEL_3_CHANNELS, 
              MODEL_LEVEL_4_BLOCKS, MODEL_LEVEL_4_CHANNELS, 
              MODEL_LEVEL_5_BLOCKS, MODEL_LEVEL_5_CHANNELS,
              MODEL_LEVEL_6_CHANNELS,MODEL_LEVEL_7_CHANNELS,
              DATA_NUM_CLASSES) # <TO DO: your code goes here> inside the parenthesis

# enable data parallelization for multi GPU systems
if (torch.cuda.device_count() > 1):
    model = nn.DataParallel(model)
print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)

################################################################################
#
# ERROR AND OPTIMIZER
#
################################################################################

# error (softmax cross entropy)
criterion = nn.CrossEntropyLoss()

# learning rate schedule
def lr_schedule(epoch):

    # linear warmup followed by 1/2 wave cosine decay
    if epoch < TRAIN_LR_INIT_EPOCHS:
        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT
    else:
        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))

    return lr

# optimizer
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, dampening=0.0, weight_decay=5e-5, nesterov=True)

################################################################################
#
# TRAINING
#
################################################################################

# start epoch
start_epoch = 0

# specify the device as the GPU if present with fallback to the CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# transfer the network to the device
model.to(device)

# load the last checkpoint
if (FILE_LOAD == True):
    checkpoint = torch.load(FILE_NAME_CHECK)
    model.load_state_dict(checkpoint['model_state_dict'])
    if (FILE_NEW_OPTIMIZER == False):
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    if (FILE_EXTEND_TRAINING == False):
        start_epoch = checkpoint['epoch'] + 1

# initialize the epoch
accuracy_best      = 0
start_time_display = time.time()
start_time_epoch   = time.time()

# cycle through the epochs
for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):

    # initialize epoch training
    model.train()
    training_loss = 0.0
    num_batches   = 0
    num_display   = 0

    # set the learning rate for the epoch
    for g in optimizer.param_groups:
        g['lr'] = lr_schedule(epoch)

    # cycle through the training data set
    for data in dataloader_train:

        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward pass, loss, backward pass and weight update
        outputs = model(inputs)
        loss    = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # update statistics
        training_loss = training_loss + loss.item()
        num_batches   = num_batches + 1
        num_display   = num_display + DATA_BATCH_SIZE

        # display intra epoch results
        if (num_display > TRAIN_INTRA_EPOCH_DISPLAY):
            num_display          = 0
            elapsed_time_display = time.time() - start_time_display
            start_time_display   = time.time()
            print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f}'.format(epoch, elapsed_time_display, lr_schedule(epoch), (training_loss / num_batches) / DATA_BATCH_SIZE), flush=True)

    # initialize epoch testing
    model.eval()
    test_correct = 0
    test_total   = 0

    # no weight update / no gradient needed
    with torch.no_grad():

        # cycle through the testing data set
        for data in dataloader_test:

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            # update test set statistics
            test_total   = test_total + labels.size(0)
            test_correct = test_correct + (predicted == labels).sum().item()

    # epoch statistics
    elapsed_time_epoch = time.time() - start_time_epoch
    start_time_epoch   = time.time()
    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)), flush=True)

    # save a checkpoint
    if (FILE_SAVE == True):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_CHECK)

    # save the best model
    accuracy_epoch = 100.0 * test_correct / test_total
    if ((FILE_SAVE == True) and (accuracy_epoch >= accuracy_best)):
        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_BEST)